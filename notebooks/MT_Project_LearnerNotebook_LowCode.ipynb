{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EaJ8AGwpM-2"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3-QehJxbp0t"
      },
      "source": [
        "### Business Context\n",
        "\n",
        "Renewable energy sources play an increasingly important role in the global energy mix, as the effort to reduce the environmental impact of energy production increases.\n",
        "\n",
        "Out of all the renewable energy alternatives, wind energy is one of the most developed technologies worldwide. The U.S Department of Energy has put together a guide to achieving operational efficiency using predictive maintenance practices.\n",
        "\n",
        "Predictive maintenance uses sensor information and analysis methods to measure and predict degradation and future component capability. The idea behind predictive maintenance is that failure patterns are predictable and if component failure can be predicted accurately and the component is replaced before it fails, the costs of operation and maintenance will be much lower.\n",
        "\n",
        "The sensors fitted across different machines involved in the process of energy generation collect data related to various environmental factors (temperature, humidity, wind speed, etc.) and additional features related to various parts of the wind turbine (gearbox, tower, blades, break, etc.). \n",
        "\n",
        "\n",
        "\n",
        "## Objective\n",
        "“ReneWind” is a company working on improving the machinery/processes involved in the production of wind energy using machine learning and has collected data of generator failure of wind turbines using sensors. They have shared a ciphered version of the data, as the data collected through sensors is confidential (the type of data collected varies with companies). Data has 40 predictors, 20000 observations in the training set and 5000 in the test set.\n",
        "\n",
        "The objective is to build various classification models, tune them, and find the best one that will help identify failures so that the generators could be repaired before failing/breaking to reduce the overall maintenance cost. \n",
        "The nature of predictions made by the classification model will translate as follows:\n",
        "\n",
        "- True positives (TP) are failures correctly predicted by the model. These will result in repairing costs.\n",
        "- False negatives (FN) are real failures where there is no detection by the model. These will result in replacement costs.\n",
        "- False positives (FP) are detections where there is no failure. These will result in inspection costs.\n",
        "\n",
        "It is given that the cost of repairing a generator is much less than the cost of replacing it, and the cost of inspection is less than the cost of repair.\n",
        "\n",
        "“1” in the target variables should be considered as “failure” and “0” represents “No failure”.\n",
        "\n",
        "## Data Description\n",
        "- The data provided is a transformed version of original data which was collected using sensors.\n",
        "- Train.csv - To be used for training and tuning of models. \n",
        "- Test.csv - To be used only for testing the performance of the final best model.\n",
        "- Both the datasets consist of 40 predictor variables and 1 target variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbHOIdlwcrqR"
      },
      "source": [
        "### **Please read the instructions carefully before starting the project.** \n",
        "This is a commented Jupyter IPython Notebook file in which all the instructions and tasks to be performed are mentioned. \n",
        "* Blanks '_______' are provided in the notebook that \n",
        "needs to be filled with an appropriate code to get the correct result. With every '_______' blank, there is a comment that briefly describes what needs to be filled in the blank space. \n",
        "* Identify the task to be performed correctly, and only then proceed to write the required code.\n",
        "* Fill the code wherever asked by the commented lines like \"# write your code here\" or \"# complete the code\". Running incomplete code may throw error.\n",
        "* Please run the codes in a sequential manner from the beginning to avoid any unnecessary errors.\n",
        "* Add the results/observations (wherever mentioned) derived from the analysis in the presentation and submit the same.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "start pre code outer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importing necessary libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "start pre-code inner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 76;\n                var nbb_unformatted_code = \"import importlib.util\\nimport sys\\nimport os\\nimport subprocess\\n\\n\\ndef installer():\\n    modules_list = [\\n        \\\"notebook\\\",\\n        \\\"matplotlib\\\",\\n        \\\"pandas\\\",\\n        \\\"seaborn\\\",\\n        \\\"numpy\\\",\\n        \\\"scipy\\\",\\n        \\\"statsmodels\\\",\\n        \\\"-U scikit-learn\\\",\\n        \\\"ipykernel\\\",\\n        \\\"nb-black\\\",\\n        \\\"importlib.util\\\",\\n        \\\"sys\\\",\\n        \\\"os\\\",\\n        \\\"importlib.util\\\",\\n        \\\"subprocess\\\",\\n        \\\"warnings\\\",\\n        \\\"statsmodels.tools.sm_exceptions\\\",\\n        \\\"matplotlib.pyplot\\\",\\n        \\\"sklearn.model_selection.train_test_split\\\",\\n        \\\"statsmodels.stats.api\\\",\\n        \\\"statsmodels.stats.outliers_influence\\\",\\n        \\\"statsmodels.api\\\",\\n        \\\"statsmodels.tools.tools\\\",\\n        \\\"sklearn.tree.DecisionTreeClassifier\\\",\\n        \\\"sklearn.tree\\\",\\n        \\\"sklearn.model_selection.GridSearchCV\\\",\\n        \\\"sklearn.metrics.f1_score\\\",\\n        \\\"sklearn.metrics.accuracy_score\\\",\\n        \\\"sklearn.metrics.recall_score\\\",\\n        \\\"sklearn.metrics.precision_score\\\",\\n        \\\"sklearn.metrics.confusion_matrix\\\",\\n        \\\"sklearn.metrics.roc_auc_score\\\",\\n        \\\"sklearn.metrics.ConfusionMatrixDisplay\\\",\\n        \\\"sklearn.metrics.precision_recall_curve\\\",\\n        \\\"sklearn.metrics.roc_curve\\\",\\n        \\\"sklearn.metrics.make_scorer\\\",\\n        \\\"sklearn.metrics\\\",  # new\\n        \\\"from sklearn.ensemble.BaggingClassifier\\\",  # new\\n        \\\"RandomForestClassifier\\\",  # new\\n        \\\"sklearn.linear_model.LogisticRegression\\\",  # new\\n        \\\"xgboost.XGBClassifier\\\", # boosting\\n        \\\"xgboost\\\",\\n        \\\"operator\\\",\\n        \\\"openpyxl\\\",\\n\\n    ]\\n    uninstalled_list = []\\n    modules_list_modified = []\\n    unchecked_formatted_list = []\\n\\n    def stringifier(string):\\n        string = string[3:15]\\n        return string\\n\\n    def sort_output(name):\\n        string = name  #\\n        # module\\n        if string == \\\"-U scikit-learn\\\":\\n            module = stringifier(string)  # string = string[2:15] #\\n            return module\\n        # now have holder=scikit-learn\\n        else:\\n            module = string\\n            return string\\n\\n    def loop_unformatted_packages(modules_list):\\n        for name in modules_list:  # '-U scikit-learn'\\n            unchecked_formatted_list.append(sort_output(name))\\n        return unchecked_formatted_list\\n\\n    unchecked_formatted_list = loop_unformatted_packages(modules_list)\\n\\n    def check_not_installed(name):\\n        \\\"\\\"\\\"\\n        Check if a package (formatted) is installed or not.\\n        \\\"\\\"\\\"\\n        if name in sys.modules:\\n            print(f\\\"{name!r} already in sys.modules\\\")\\n        elif (spec := importlib.util.find_spec(name)) is not None:\\n            # If you choose to perform the actual import ..\\n            module = importlib.util.module_from_spec(spec)\\n            sys.modules[name] = module\\n            spec.loader.exec_module(module)\\n            print(f\\\"{name!r} has been imported\\\")\\n        else:\\n            print(f\\\"can't find the {name!r} module\\\")\\n            return name\\n\\n    def loop_formatted_packages(unchecked_formatted_list, uninstalled_list):\\n        \\\"\\\"\\\"\\n        takes:\\n        - var unchecked_formatted_list\\n        - var uninstalled_list\\n        calls:\\n        - function check_not_installed\\n        \\\"\\\"\\\"\\n        for name in unchecked_formatted_list:  # '-U scikit-learn'\\n            if check_not_installed(name):\\n                uninstalled_list.append(name)\\n        return uninstalled_list\\n\\n    uninstalled_list = loop_formatted_packages(\\n        unchecked_formatted_list, uninstalled_list\\n    )\\n\\n    print(\\\"\\\\nuninstalled modules are:\\\", uninstalled_list, \\\"\\\\n\\\")\\n\\n    newly_installed = []\\n\\n    def installer(uninstalled_list):\\n        for name in uninstalled_list:\\n            if name == \\\"scikit-learn\\\":\\n                name2 = \\\"-U \\\" + name\\n            os.system(\\\"pip install {}\\\".format(name2))\\n            print(name, \\\"module has been installed\\\")\\n            newly_installed.append(name)\\n        print(\\\"\\\\n\\\", newly_installed, \\\"modules have just been installed\\\")\\n\\n    installer(uninstalled_list)\";\n                var nbb_formatted_code = \"import importlib.util\\nimport sys\\nimport os\\nimport subprocess\\n\\n\\ndef installer():\\n    modules_list = [\\n        \\\"notebook\\\",\\n        \\\"matplotlib\\\",\\n        \\\"pandas\\\",\\n        \\\"seaborn\\\",\\n        \\\"numpy\\\",\\n        \\\"scipy\\\",\\n        \\\"statsmodels\\\",\\n        \\\"-U scikit-learn\\\",\\n        \\\"ipykernel\\\",\\n        \\\"nb-black\\\",\\n        \\\"importlib.util\\\",\\n        \\\"sys\\\",\\n        \\\"os\\\",\\n        \\\"importlib.util\\\",\\n        \\\"subprocess\\\",\\n        \\\"warnings\\\",\\n        \\\"statsmodels.tools.sm_exceptions\\\",\\n        \\\"matplotlib.pyplot\\\",\\n        \\\"sklearn.model_selection.train_test_split\\\",\\n        \\\"statsmodels.stats.api\\\",\\n        \\\"statsmodels.stats.outliers_influence\\\",\\n        \\\"statsmodels.api\\\",\\n        \\\"statsmodels.tools.tools\\\",\\n        \\\"sklearn.tree.DecisionTreeClassifier\\\",\\n        \\\"sklearn.tree\\\",\\n        \\\"sklearn.model_selection.GridSearchCV\\\",\\n        \\\"sklearn.metrics.f1_score\\\",\\n        \\\"sklearn.metrics.accuracy_score\\\",\\n        \\\"sklearn.metrics.recall_score\\\",\\n        \\\"sklearn.metrics.precision_score\\\",\\n        \\\"sklearn.metrics.confusion_matrix\\\",\\n        \\\"sklearn.metrics.roc_auc_score\\\",\\n        \\\"sklearn.metrics.ConfusionMatrixDisplay\\\",\\n        \\\"sklearn.metrics.precision_recall_curve\\\",\\n        \\\"sklearn.metrics.roc_curve\\\",\\n        \\\"sklearn.metrics.make_scorer\\\",\\n        \\\"sklearn.metrics\\\",  # new\\n        \\\"from sklearn.ensemble.BaggingClassifier\\\",  # new\\n        \\\"RandomForestClassifier\\\",  # new\\n        \\\"sklearn.linear_model.LogisticRegression\\\",  # new\\n        \\\"xgboost.XGBClassifier\\\",  # boosting\\n        \\\"xgboost\\\",\\n        \\\"operator\\\",\\n        \\\"openpyxl\\\",\\n    ]\\n    uninstalled_list = []\\n    modules_list_modified = []\\n    unchecked_formatted_list = []\\n\\n    def stringifier(string):\\n        string = string[3:15]\\n        return string\\n\\n    def sort_output(name):\\n        string = name  #\\n        # module\\n        if string == \\\"-U scikit-learn\\\":\\n            module = stringifier(string)  # string = string[2:15] #\\n            return module\\n        # now have holder=scikit-learn\\n        else:\\n            module = string\\n            return string\\n\\n    def loop_unformatted_packages(modules_list):\\n        for name in modules_list:  # '-U scikit-learn'\\n            unchecked_formatted_list.append(sort_output(name))\\n        return unchecked_formatted_list\\n\\n    unchecked_formatted_list = loop_unformatted_packages(modules_list)\\n\\n    def check_not_installed(name):\\n        \\\"\\\"\\\"\\n        Check if a package (formatted) is installed or not.\\n        \\\"\\\"\\\"\\n        if name in sys.modules:\\n            print(f\\\"{name!r} already in sys.modules\\\")\\n        elif (spec := importlib.util.find_spec(name)) is not None:\\n            # If you choose to perform the actual import ..\\n            module = importlib.util.module_from_spec(spec)\\n            sys.modules[name] = module\\n            spec.loader.exec_module(module)\\n            print(f\\\"{name!r} has been imported\\\")\\n        else:\\n            print(f\\\"can't find the {name!r} module\\\")\\n            return name\\n\\n    def loop_formatted_packages(unchecked_formatted_list, uninstalled_list):\\n        \\\"\\\"\\\"\\n        takes:\\n        - var unchecked_formatted_list\\n        - var uninstalled_list\\n        calls:\\n        - function check_not_installed\\n        \\\"\\\"\\\"\\n        for name in unchecked_formatted_list:  # '-U scikit-learn'\\n            if check_not_installed(name):\\n                uninstalled_list.append(name)\\n        return uninstalled_list\\n\\n    uninstalled_list = loop_formatted_packages(\\n        unchecked_formatted_list, uninstalled_list\\n    )\\n\\n    print(\\\"\\\\nuninstalled modules are:\\\", uninstalled_list, \\\"\\\\n\\\")\\n\\n    newly_installed = []\\n\\n    def installer(uninstalled_list):\\n        for name in uninstalled_list:\\n            if name == \\\"scikit-learn\\\":\\n                name2 = \\\"-U \\\" + name\\n            os.system(\\\"pip install {}\\\".format(name2))\\n            print(name, \\\"module has been installed\\\")\\n            newly_installed.append(name)\\n        print(\\\"\\\\n\\\", newly_installed, \\\"modules have just been installed\\\")\\n\\n    installer(uninstalled_list)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 76;\n                var nbb_unformatted_code = \"import importlib.util\\nimport sys\\nimport os\\nimport subprocess\\n\\n\\ndef installer():\\n    modules_list = [\\n        \\\"notebook\\\",\\n        \\\"matplotlib\\\",\\n        \\\"pandas\\\",\\n        \\\"seaborn\\\",\\n        \\\"numpy\\\",\\n        \\\"scipy\\\",\\n        \\\"statsmodels\\\",\\n        \\\"-U scikit-learn\\\",\\n        \\\"ipykernel\\\",\\n        \\\"nb-black\\\",\\n        \\\"importlib.util\\\",\\n        \\\"sys\\\",\\n        \\\"os\\\",\\n        \\\"importlib.util\\\",\\n        \\\"subprocess\\\",\\n        \\\"warnings\\\",\\n        \\\"statsmodels.tools.sm_exceptions\\\",\\n        \\\"matplotlib.pyplot\\\",\\n        \\\"sklearn.model_selection.train_test_split\\\",\\n        \\\"statsmodels.stats.api\\\",\\n        \\\"statsmodels.stats.outliers_influence\\\",\\n        \\\"statsmodels.api\\\",\\n        \\\"statsmodels.tools.tools\\\",\\n        \\\"sklearn.tree.DecisionTreeClassifier\\\",\\n        \\\"sklearn.tree\\\",\\n        \\\"sklearn.model_selection.GridSearchCV\\\",\\n        \\\"sklearn.metrics.f1_score\\\",\\n        \\\"sklearn.metrics.accuracy_score\\\",\\n        \\\"sklearn.metrics.recall_score\\\",\\n        \\\"sklearn.metrics.precision_score\\\",\\n        \\\"sklearn.metrics.confusion_matrix\\\",\\n        \\\"sklearn.metrics.roc_auc_score\\\",\\n        \\\"sklearn.metrics.ConfusionMatrixDisplay\\\",\\n        \\\"sklearn.metrics.precision_recall_curve\\\",\\n        \\\"sklearn.metrics.roc_curve\\\",\\n        \\\"sklearn.metrics.make_scorer\\\",\\n        \\\"sklearn.metrics\\\",  # new\\n        \\\"from sklearn.ensemble.BaggingClassifier\\\",  # new\\n        \\\"RandomForestClassifier\\\",  # new\\n        \\\"sklearn.linear_model.LogisticRegression\\\",  # new\\n        \\\"xgboost.XGBClassifier\\\", # boosting\\n        \\\"xgboost\\\",\\n        \\\"operator\\\",\\n        \\\"openpyxl\\\",\\n\\n    ]\\n    uninstalled_list = []\\n    modules_list_modified = []\\n    unchecked_formatted_list = []\\n\\n    def stringifier(string):\\n        string = string[3:15]\\n        return string\\n\\n    def sort_output(name):\\n        string = name  #\\n        # module\\n        if string == \\\"-U scikit-learn\\\":\\n            module = stringifier(string)  # string = string[2:15] #\\n            return module\\n        # now have holder=scikit-learn\\n        else:\\n            module = string\\n            return string\\n\\n    def loop_unformatted_packages(modules_list):\\n        for name in modules_list:  # '-U scikit-learn'\\n            unchecked_formatted_list.append(sort_output(name))\\n        return unchecked_formatted_list\\n\\n    unchecked_formatted_list = loop_unformatted_packages(modules_list)\\n\\n    def check_not_installed(name):\\n        \\\"\\\"\\\"\\n        Check if a package (formatted) is installed or not.\\n        \\\"\\\"\\\"\\n        if name in sys.modules:\\n            print(f\\\"{name!r} already in sys.modules\\\")\\n        elif (spec := importlib.util.find_spec(name)) is not None:\\n            # If you choose to perform the actual import ..\\n            module = importlib.util.module_from_spec(spec)\\n            sys.modules[name] = module\\n            spec.loader.exec_module(module)\\n            print(f\\\"{name!r} has been imported\\\")\\n        else:\\n            print(f\\\"can't find the {name!r} module\\\")\\n            return name\\n\\n    def loop_formatted_packages(unchecked_formatted_list, uninstalled_list):\\n        \\\"\\\"\\\"\\n        takes:\\n        - var unchecked_formatted_list\\n        - var uninstalled_list\\n        calls:\\n        - function check_not_installed\\n        \\\"\\\"\\\"\\n        for name in unchecked_formatted_list:  # '-U scikit-learn'\\n            if check_not_installed(name):\\n                uninstalled_list.append(name)\\n        return uninstalled_list\\n\\n    uninstalled_list = loop_formatted_packages(\\n        unchecked_formatted_list, uninstalled_list\\n    )\\n\\n    print(\\\"\\\\nuninstalled modules are:\\\", uninstalled_list, \\\"\\\\n\\\")\\n\\n    newly_installed = []\\n\\n    def installer(uninstalled_list):\\n        for name in uninstalled_list:\\n            if name == \\\"scikit-learn\\\":\\n                name2 = \\\"-U \\\" + name\\n            os.system(\\\"pip install {}\\\".format(name2))\\n            print(name, \\\"module has been installed\\\")\\n            newly_installed.append(name)\\n        print(\\\"\\\\n\\\", newly_installed, \\\"modules have just been installed\\\")\\n\\n    installer(uninstalled_list)\";\n                var nbb_formatted_code = \"import importlib.util\\nimport sys\\nimport os\\nimport subprocess\\n\\n\\ndef installer():\\n    modules_list = [\\n        \\\"notebook\\\",\\n        \\\"matplotlib\\\",\\n        \\\"pandas\\\",\\n        \\\"seaborn\\\",\\n        \\\"numpy\\\",\\n        \\\"scipy\\\",\\n        \\\"statsmodels\\\",\\n        \\\"-U scikit-learn\\\",\\n        \\\"ipykernel\\\",\\n        \\\"nb-black\\\",\\n        \\\"importlib.util\\\",\\n        \\\"sys\\\",\\n        \\\"os\\\",\\n        \\\"importlib.util\\\",\\n        \\\"subprocess\\\",\\n        \\\"warnings\\\",\\n        \\\"statsmodels.tools.sm_exceptions\\\",\\n        \\\"matplotlib.pyplot\\\",\\n        \\\"sklearn.model_selection.train_test_split\\\",\\n        \\\"statsmodels.stats.api\\\",\\n        \\\"statsmodels.stats.outliers_influence\\\",\\n        \\\"statsmodels.api\\\",\\n        \\\"statsmodels.tools.tools\\\",\\n        \\\"sklearn.tree.DecisionTreeClassifier\\\",\\n        \\\"sklearn.tree\\\",\\n        \\\"sklearn.model_selection.GridSearchCV\\\",\\n        \\\"sklearn.metrics.f1_score\\\",\\n        \\\"sklearn.metrics.accuracy_score\\\",\\n        \\\"sklearn.metrics.recall_score\\\",\\n        \\\"sklearn.metrics.precision_score\\\",\\n        \\\"sklearn.metrics.confusion_matrix\\\",\\n        \\\"sklearn.metrics.roc_auc_score\\\",\\n        \\\"sklearn.metrics.ConfusionMatrixDisplay\\\",\\n        \\\"sklearn.metrics.precision_recall_curve\\\",\\n        \\\"sklearn.metrics.roc_curve\\\",\\n        \\\"sklearn.metrics.make_scorer\\\",\\n        \\\"sklearn.metrics\\\",  # new\\n        \\\"from sklearn.ensemble.BaggingClassifier\\\",  # new\\n        \\\"RandomForestClassifier\\\",  # new\\n        \\\"sklearn.linear_model.LogisticRegression\\\",  # new\\n        \\\"xgboost.XGBClassifier\\\",  # boosting\\n        \\\"xgboost\\\",\\n        \\\"operator\\\",\\n        \\\"openpyxl\\\",\\n    ]\\n    uninstalled_list = []\\n    modules_list_modified = []\\n    unchecked_formatted_list = []\\n\\n    def stringifier(string):\\n        string = string[3:15]\\n        return string\\n\\n    def sort_output(name):\\n        string = name  #\\n        # module\\n        if string == \\\"-U scikit-learn\\\":\\n            module = stringifier(string)  # string = string[2:15] #\\n            return module\\n        # now have holder=scikit-learn\\n        else:\\n            module = string\\n            return string\\n\\n    def loop_unformatted_packages(modules_list):\\n        for name in modules_list:  # '-U scikit-learn'\\n            unchecked_formatted_list.append(sort_output(name))\\n        return unchecked_formatted_list\\n\\n    unchecked_formatted_list = loop_unformatted_packages(modules_list)\\n\\n    def check_not_installed(name):\\n        \\\"\\\"\\\"\\n        Check if a package (formatted) is installed or not.\\n        \\\"\\\"\\\"\\n        if name in sys.modules:\\n            print(f\\\"{name!r} already in sys.modules\\\")\\n        elif (spec := importlib.util.find_spec(name)) is not None:\\n            # If you choose to perform the actual import ..\\n            module = importlib.util.module_from_spec(spec)\\n            sys.modules[name] = module\\n            spec.loader.exec_module(module)\\n            print(f\\\"{name!r} has been imported\\\")\\n        else:\\n            print(f\\\"can't find the {name!r} module\\\")\\n            return name\\n\\n    def loop_formatted_packages(unchecked_formatted_list, uninstalled_list):\\n        \\\"\\\"\\\"\\n        takes:\\n        - var unchecked_formatted_list\\n        - var uninstalled_list\\n        calls:\\n        - function check_not_installed\\n        \\\"\\\"\\\"\\n        for name in unchecked_formatted_list:  # '-U scikit-learn'\\n            if check_not_installed(name):\\n                uninstalled_list.append(name)\\n        return uninstalled_list\\n\\n    uninstalled_list = loop_formatted_packages(\\n        unchecked_formatted_list, uninstalled_list\\n    )\\n\\n    print(\\\"\\\\nuninstalled modules are:\\\", uninstalled_list, \\\"\\\\n\\\")\\n\\n    newly_installed = []\\n\\n    def installer(uninstalled_list):\\n        for name in uninstalled_list:\\n            if name == \\\"scikit-learn\\\":\\n                name2 = \\\"-U \\\" + name\\n            os.system(\\\"pip install {}\\\".format(name2))\\n            print(name, \\\"module has been installed\\\")\\n            newly_installed.append(name)\\n        print(\\\"\\\\n\\\", newly_installed, \\\"modules have just been installed\\\")\\n\\n    installer(uninstalled_list)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 76;\n                var nbb_unformatted_code = \"import importlib.util\\nimport sys\\nimport os\\nimport subprocess\\n\\n\\ndef installer():\\n    modules_list = [\\n        \\\"notebook\\\",\\n        \\\"matplotlib\\\",\\n        \\\"pandas\\\",\\n        \\\"seaborn\\\",\\n        \\\"numpy\\\",\\n        \\\"scipy\\\",\\n        \\\"statsmodels\\\",\\n        \\\"-U scikit-learn\\\",\\n        \\\"ipykernel\\\",\\n        \\\"nb-black\\\",\\n        \\\"importlib.util\\\",\\n        \\\"sys\\\",\\n        \\\"os\\\",\\n        \\\"importlib.util\\\",\\n        \\\"subprocess\\\",\\n        \\\"warnings\\\",\\n        \\\"statsmodels.tools.sm_exceptions\\\",\\n        \\\"matplotlib.pyplot\\\",\\n        \\\"sklearn.model_selection.train_test_split\\\",\\n        \\\"statsmodels.stats.api\\\",\\n        \\\"statsmodels.stats.outliers_influence\\\",\\n        \\\"statsmodels.api\\\",\\n        \\\"statsmodels.tools.tools\\\",\\n        \\\"sklearn.tree.DecisionTreeClassifier\\\",\\n        \\\"sklearn.tree\\\",\\n        \\\"sklearn.model_selection.GridSearchCV\\\",\\n        \\\"sklearn.metrics.f1_score\\\",\\n        \\\"sklearn.metrics.accuracy_score\\\",\\n        \\\"sklearn.metrics.recall_score\\\",\\n        \\\"sklearn.metrics.precision_score\\\",\\n        \\\"sklearn.metrics.confusion_matrix\\\",\\n        \\\"sklearn.metrics.roc_auc_score\\\",\\n        \\\"sklearn.metrics.ConfusionMatrixDisplay\\\",\\n        \\\"sklearn.metrics.precision_recall_curve\\\",\\n        \\\"sklearn.metrics.roc_curve\\\",\\n        \\\"sklearn.metrics.make_scorer\\\",\\n        \\\"sklearn.metrics\\\",  # new\\n        \\\"from sklearn.ensemble.BaggingClassifier\\\",  # new\\n        \\\"RandomForestClassifier\\\",  # new\\n        \\\"sklearn.linear_model.LogisticRegression\\\",  # new\\n        \\\"xgboost.XGBClassifier\\\", # boosting\\n        \\\"xgboost\\\",\\n        \\\"operator\\\",\\n        \\\"openpyxl\\\",\\n\\n    ]\\n    uninstalled_list = []\\n    modules_list_modified = []\\n    unchecked_formatted_list = []\\n\\n    def stringifier(string):\\n        string = string[3:15]\\n        return string\\n\\n    def sort_output(name):\\n        string = name  #\\n        # module\\n        if string == \\\"-U scikit-learn\\\":\\n            module = stringifier(string)  # string = string[2:15] #\\n            return module\\n        # now have holder=scikit-learn\\n        else:\\n            module = string\\n            return string\\n\\n    def loop_unformatted_packages(modules_list):\\n        for name in modules_list:  # '-U scikit-learn'\\n            unchecked_formatted_list.append(sort_output(name))\\n        return unchecked_formatted_list\\n\\n    unchecked_formatted_list = loop_unformatted_packages(modules_list)\\n\\n    def check_not_installed(name):\\n        \\\"\\\"\\\"\\n        Check if a package (formatted) is installed or not.\\n        \\\"\\\"\\\"\\n        if name in sys.modules:\\n            print(f\\\"{name!r} already in sys.modules\\\")\\n        elif (spec := importlib.util.find_spec(name)) is not None:\\n            # If you choose to perform the actual import ..\\n            module = importlib.util.module_from_spec(spec)\\n            sys.modules[name] = module\\n            spec.loader.exec_module(module)\\n            print(f\\\"{name!r} has been imported\\\")\\n        else:\\n            print(f\\\"can't find the {name!r} module\\\")\\n            return name\\n\\n    def loop_formatted_packages(unchecked_formatted_list, uninstalled_list):\\n        \\\"\\\"\\\"\\n        takes:\\n        - var unchecked_formatted_list\\n        - var uninstalled_list\\n        calls:\\n        - function check_not_installed\\n        \\\"\\\"\\\"\\n        for name in unchecked_formatted_list:  # '-U scikit-learn'\\n            if check_not_installed(name):\\n                uninstalled_list.append(name)\\n        return uninstalled_list\\n\\n    uninstalled_list = loop_formatted_packages(\\n        unchecked_formatted_list, uninstalled_list\\n    )\\n\\n    print(\\\"\\\\nuninstalled modules are:\\\", uninstalled_list, \\\"\\\\n\\\")\\n\\n    newly_installed = []\\n\\n    def installer(uninstalled_list):\\n        for name in uninstalled_list:\\n            if name == \\\"scikit-learn\\\":\\n                name2 = \\\"-U \\\" + name\\n            os.system(\\\"pip install {}\\\".format(name2))\\n            print(name, \\\"module has been installed\\\")\\n            newly_installed.append(name)\\n        print(\\\"\\\\n\\\", newly_installed, \\\"modules have just been installed\\\")\\n\\n    installer(uninstalled_list)\";\n                var nbb_formatted_code = \"import importlib.util\\nimport sys\\nimport os\\nimport subprocess\\n\\n\\ndef installer():\\n    modules_list = [\\n        \\\"notebook\\\",\\n        \\\"matplotlib\\\",\\n        \\\"pandas\\\",\\n        \\\"seaborn\\\",\\n        \\\"numpy\\\",\\n        \\\"scipy\\\",\\n        \\\"statsmodels\\\",\\n        \\\"-U scikit-learn\\\",\\n        \\\"ipykernel\\\",\\n        \\\"nb-black\\\",\\n        \\\"importlib.util\\\",\\n        \\\"sys\\\",\\n        \\\"os\\\",\\n        \\\"importlib.util\\\",\\n        \\\"subprocess\\\",\\n        \\\"warnings\\\",\\n        \\\"statsmodels.tools.sm_exceptions\\\",\\n        \\\"matplotlib.pyplot\\\",\\n        \\\"sklearn.model_selection.train_test_split\\\",\\n        \\\"statsmodels.stats.api\\\",\\n        \\\"statsmodels.stats.outliers_influence\\\",\\n        \\\"statsmodels.api\\\",\\n        \\\"statsmodels.tools.tools\\\",\\n        \\\"sklearn.tree.DecisionTreeClassifier\\\",\\n        \\\"sklearn.tree\\\",\\n        \\\"sklearn.model_selection.GridSearchCV\\\",\\n        \\\"sklearn.metrics.f1_score\\\",\\n        \\\"sklearn.metrics.accuracy_score\\\",\\n        \\\"sklearn.metrics.recall_score\\\",\\n        \\\"sklearn.metrics.precision_score\\\",\\n        \\\"sklearn.metrics.confusion_matrix\\\",\\n        \\\"sklearn.metrics.roc_auc_score\\\",\\n        \\\"sklearn.metrics.ConfusionMatrixDisplay\\\",\\n        \\\"sklearn.metrics.precision_recall_curve\\\",\\n        \\\"sklearn.metrics.roc_curve\\\",\\n        \\\"sklearn.metrics.make_scorer\\\",\\n        \\\"sklearn.metrics\\\",  # new\\n        \\\"from sklearn.ensemble.BaggingClassifier\\\",  # new\\n        \\\"RandomForestClassifier\\\",  # new\\n        \\\"sklearn.linear_model.LogisticRegression\\\",  # new\\n        \\\"xgboost.XGBClassifier\\\",  # boosting\\n        \\\"xgboost\\\",\\n        \\\"operator\\\",\\n        \\\"openpyxl\\\",\\n    ]\\n    uninstalled_list = []\\n    modules_list_modified = []\\n    unchecked_formatted_list = []\\n\\n    def stringifier(string):\\n        string = string[3:15]\\n        return string\\n\\n    def sort_output(name):\\n        string = name  #\\n        # module\\n        if string == \\\"-U scikit-learn\\\":\\n            module = stringifier(string)  # string = string[2:15] #\\n            return module\\n        # now have holder=scikit-learn\\n        else:\\n            module = string\\n            return string\\n\\n    def loop_unformatted_packages(modules_list):\\n        for name in modules_list:  # '-U scikit-learn'\\n            unchecked_formatted_list.append(sort_output(name))\\n        return unchecked_formatted_list\\n\\n    unchecked_formatted_list = loop_unformatted_packages(modules_list)\\n\\n    def check_not_installed(name):\\n        \\\"\\\"\\\"\\n        Check if a package (formatted) is installed or not.\\n        \\\"\\\"\\\"\\n        if name in sys.modules:\\n            print(f\\\"{name!r} already in sys.modules\\\")\\n        elif (spec := importlib.util.find_spec(name)) is not None:\\n            # If you choose to perform the actual import ..\\n            module = importlib.util.module_from_spec(spec)\\n            sys.modules[name] = module\\n            spec.loader.exec_module(module)\\n            print(f\\\"{name!r} has been imported\\\")\\n        else:\\n            print(f\\\"can't find the {name!r} module\\\")\\n            return name\\n\\n    def loop_formatted_packages(unchecked_formatted_list, uninstalled_list):\\n        \\\"\\\"\\\"\\n        takes:\\n        - var unchecked_formatted_list\\n        - var uninstalled_list\\n        calls:\\n        - function check_not_installed\\n        \\\"\\\"\\\"\\n        for name in unchecked_formatted_list:  # '-U scikit-learn'\\n            if check_not_installed(name):\\n                uninstalled_list.append(name)\\n        return uninstalled_list\\n\\n    uninstalled_list = loop_formatted_packages(\\n        unchecked_formatted_list, uninstalled_list\\n    )\\n\\n    print(\\\"\\\\nuninstalled modules are:\\\", uninstalled_list, \\\"\\\\n\\\")\\n\\n    newly_installed = []\\n\\n    def installer(uninstalled_list):\\n        for name in uninstalled_list:\\n            if name == \\\"scikit-learn\\\":\\n                name2 = \\\"-U \\\" + name\\n            os.system(\\\"pip install {}\\\".format(name2))\\n            print(name, \\\"module has been installed\\\")\\n            newly_installed.append(name)\\n        print(\\\"\\\\n\\\", newly_installed, \\\"modules have just been installed\\\")\\n\\n    installer(uninstalled_list)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import importlib.util\n",
        "import sys\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "\n",
        "def installer():\n",
        "    modules_list = [\n",
        "        \"notebook\",\n",
        "        \"matplotlib\",\n",
        "        \"pandas\",\n",
        "        \"seaborn\",\n",
        "        \"numpy\",\n",
        "        \"scipy\",\n",
        "        \"statsmodels\",\n",
        "        \"-U scikit-learn\",\n",
        "        \"ipykernel\",\n",
        "        \"nb-black\",\n",
        "        \"importlib.util\",\n",
        "        \"sys\",\n",
        "        \"os\",\n",
        "        \"importlib.util\",\n",
        "        \"subprocess\",\n",
        "        \"warnings\",\n",
        "        \"statsmodels.tools.sm_exceptions\",\n",
        "        \"matplotlib.pyplot\",\n",
        "        \"sklearn.model_selection.train_test_split\",\n",
        "        \"statsmodels.stats.api\",\n",
        "        \"statsmodels.stats.outliers_influence\",\n",
        "        \"statsmodels.api\",\n",
        "        \"statsmodels.tools.tools\",\n",
        "        \"sklearn.tree.DecisionTreeClassifier\",\n",
        "        \"sklearn.tree\",\n",
        "        \"sklearn.model_selection.GridSearchCV\",\n",
        "        \"sklearn.metrics.f1_score\",\n",
        "        \"sklearn.metrics.accuracy_score\",\n",
        "        \"sklearn.metrics.recall_score\",\n",
        "        \"sklearn.metrics.precision_score\",\n",
        "        \"sklearn.metrics.confusion_matrix\",\n",
        "        \"sklearn.metrics.roc_auc_score\",\n",
        "        \"sklearn.metrics.ConfusionMatrixDisplay\",\n",
        "        \"sklearn.metrics.precision_recall_curve\",\n",
        "        \"sklearn.metrics.roc_curve\",\n",
        "        \"sklearn.metrics.make_scorer\",\n",
        "        \"sklearn.metrics\",  # new\n",
        "        \"from sklearn.ensemble.BaggingClassifier\",  # new\n",
        "        \"RandomForestClassifier\",  # new\n",
        "        \"sklearn.linear_model.LogisticRegression\",  # new\n",
        "        \"xgboost.XGBClassifier\", # boosting\n",
        "        \"xgboost\",\n",
        "        \"operator\",\n",
        "        \"openpyxl\",\n",
        "\n",
        "    ]\n",
        "    uninstalled_list = []\n",
        "    modules_list_modified = []\n",
        "    unchecked_formatted_list = []\n",
        "\n",
        "    def stringifier(string):\n",
        "        string = string[3:15]\n",
        "        return string\n",
        "\n",
        "    def sort_output(name):\n",
        "        string = name  #\n",
        "        # module\n",
        "        if string == \"-U scikit-learn\":\n",
        "            module = stringifier(string)  # string = string[2:15] #\n",
        "            return module\n",
        "        # now have holder=scikit-learn\n",
        "        else:\n",
        "            module = string\n",
        "            return string\n",
        "\n",
        "    def loop_unformatted_packages(modules_list):\n",
        "        for name in modules_list:  # '-U scikit-learn'\n",
        "            unchecked_formatted_list.append(sort_output(name))\n",
        "        return unchecked_formatted_list\n",
        "\n",
        "    unchecked_formatted_list = loop_unformatted_packages(modules_list)\n",
        "\n",
        "    def check_not_installed(name):\n",
        "        \"\"\"\n",
        "        Check if a package (formatted) is installed or not.\n",
        "        \"\"\"\n",
        "        if name in sys.modules:\n",
        "            print(f\"{name!r} already in sys.modules\")\n",
        "        elif (spec := importlib.util.find_spec(name)) is not None:\n",
        "            # If you choose to perform the actual import ..\n",
        "            module = importlib.util.module_from_spec(spec)\n",
        "            sys.modules[name] = module\n",
        "            spec.loader.exec_module(module)\n",
        "            print(f\"{name!r} has been imported\")\n",
        "        else:\n",
        "            print(f\"can't find the {name!r} module\")\n",
        "            return name\n",
        "\n",
        "    def loop_formatted_packages(unchecked_formatted_list, uninstalled_list):\n",
        "        \"\"\"\n",
        "        takes:\n",
        "        - var unchecked_formatted_list\n",
        "        - var uninstalled_list\n",
        "        calls:\n",
        "        - function check_not_installed\n",
        "        \"\"\"\n",
        "        for name in unchecked_formatted_list:  # '-U scikit-learn'\n",
        "            if check_not_installed(name):\n",
        "                uninstalled_list.append(name)\n",
        "        return uninstalled_list\n",
        "\n",
        "    uninstalled_list = loop_formatted_packages(\n",
        "        unchecked_formatted_list, uninstalled_list\n",
        "    )\n",
        "\n",
        "    print(\"\\nuninstalled modules are:\", uninstalled_list, \"\\n\")\n",
        "\n",
        "    newly_installed = []\n",
        "\n",
        "    def installer(uninstalled_list):\n",
        "        for name in uninstalled_list:\n",
        "            if name == \"scikit-learn\":\n",
        "                name2 = \"-U \" + name\n",
        "            os.system(\"pip install {}\".format(name2))\n",
        "            print(name, \"module has been installed\")\n",
        "            newly_installed.append(name)\n",
        "        print(\"\\n\", newly_installed, \"modules have just been installed\")\n",
        "\n",
        "    installer(uninstalled_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 77;\n                var nbb_unformatted_code = \"# installer()\";\n                var nbb_formatted_code = \"# installer()\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 77;\n                var nbb_unformatted_code = \"# installer()\";\n                var nbb_formatted_code = \"# installer()\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 77;\n                var nbb_unformatted_code = \"# installer()\";\n                var nbb_formatted_code = \"# installer()\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# installer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 78;\n                var nbb_unformatted_code = \"# uninstalled_list_2 = ['pandas', 'matplotlib']\\n# installer(uninstalled_list_2)\";\n                var nbb_formatted_code = \"# uninstalled_list_2 = ['pandas', 'matplotlib']\\n# installer(uninstalled_list_2)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 78;\n                var nbb_unformatted_code = \"# uninstalled_list_2 = ['pandas', 'matplotlib']\\n# installer(uninstalled_list_2)\";\n                var nbb_formatted_code = \"# uninstalled_list_2 = ['pandas', 'matplotlib']\\n# installer(uninstalled_list_2)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 78;\n                var nbb_unformatted_code = \"# uninstalled_list_2 = ['pandas', 'matplotlib']\\n# installer(uninstalled_list_2)\";\n                var nbb_formatted_code = \"# uninstalled_list_2 = ['pandas', 'matplotlib']\\n# installer(uninstalled_list_2)\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# uninstalled_list_2 = ['pandas', 'matplotlib']\n",
        "# installer(uninstalled_list_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 79;\n                var nbb_unformatted_code = \"import operator\";\n                var nbb_formatted_code = \"import operator\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 79;\n                var nbb_unformatted_code = \"import operator\";\n                var nbb_formatted_code = \"import operator\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 79;\n                var nbb_unformatted_code = \"import operator\";\n                var nbb_formatted_code = \"import operator\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import operator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 80;\n                var nbb_unformatted_code = \"# To help with reading and manipulating data\\nimport pandas as pd\\nimport numpy as np\\n\\n# To help with data visualization\\n%matplotlib inline\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# To be used for missing value imputation\\nfrom sklearn.impute import SimpleImputer\\n\\n# To help with model building\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.ensemble import (\\n    AdaBoostClassifier,\\n    GradientBoostingClassifier,\\n    RandomForestClassifier,\\n    BaggingClassifier,\\n)\\nfrom xgboost import XGBClassifier\\n\\n# To get different metric scores, and split data\\nfrom sklearn import metrics\\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\\nfrom sklearn.metrics import (\\n    f1_score,\\n    accuracy_score,\\n    recall_score,\\n    precision_score,\\n    confusion_matrix,\\n    roc_auc_score,\\n    # plot_confusion_matrix,\\n    ConfusionMatrixDisplay,\\n)\\n\\n# To be used for data scaling and one hot encoding\\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\\n\\n# To be used for tuning the model\\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\\n\\n# To be used for creating pipelines and personalizing them\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.compose import ColumnTransformer\\n\\n# To define maximum number of columns to be displayed in a dataframe\\npd.set_option(\\\"display.max_columns\\\", None)\\n\\n# To supress scientific notations for a dataframe\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.3f\\\" % x)\\n\\n# To supress warnings\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# This will help in making the Python code more structured automatically (good coding practice)\\n#%load_ext nb_black\";\n                var nbb_formatted_code = \"# To help with reading and manipulating data\\nimport pandas as pd\\nimport numpy as np\\n\\n# To help with data visualization\\n%matplotlib inline\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# To be used for missing value imputation\\nfrom sklearn.impute import SimpleImputer\\n\\n# To help with model building\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.ensemble import (\\n    AdaBoostClassifier,\\n    GradientBoostingClassifier,\\n    RandomForestClassifier,\\n    BaggingClassifier,\\n)\\nfrom xgboost import XGBClassifier\\n\\n# To get different metric scores, and split data\\nfrom sklearn import metrics\\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\\nfrom sklearn.metrics import (\\n    f1_score,\\n    accuracy_score,\\n    recall_score,\\n    precision_score,\\n    confusion_matrix,\\n    roc_auc_score,\\n    # plot_confusion_matrix,\\n    ConfusionMatrixDisplay,\\n)\\n\\n# To be used for data scaling and one hot encoding\\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\\n\\n# To be used for tuning the model\\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\\n\\n# To be used for creating pipelines and personalizing them\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.compose import ColumnTransformer\\n\\n# To define maximum number of columns to be displayed in a dataframe\\npd.set_option(\\\"display.max_columns\\\", None)\\n\\n# To supress scientific notations for a dataframe\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.3f\\\" % x)\\n\\n# To supress warnings\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# This will help in making the Python code more structured automatically (good coding practice)\\n# %load_ext nb_black\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 80;\n                var nbb_unformatted_code = \"# To help with reading and manipulating data\\nimport pandas as pd\\nimport numpy as np\\n\\n# To help with data visualization\\n%matplotlib inline\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# To be used for missing value imputation\\nfrom sklearn.impute import SimpleImputer\\n\\n# To help with model building\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.ensemble import (\\n    AdaBoostClassifier,\\n    GradientBoostingClassifier,\\n    RandomForestClassifier,\\n    BaggingClassifier,\\n)\\nfrom xgboost import XGBClassifier\\n\\n# To get different metric scores, and split data\\nfrom sklearn import metrics\\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\\nfrom sklearn.metrics import (\\n    f1_score,\\n    accuracy_score,\\n    recall_score,\\n    precision_score,\\n    confusion_matrix,\\n    roc_auc_score,\\n    # plot_confusion_matrix,\\n    ConfusionMatrixDisplay,\\n)\\n\\n# To be used for data scaling and one hot encoding\\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\\n\\n# To be used for tuning the model\\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\\n\\n# To be used for creating pipelines and personalizing them\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.compose import ColumnTransformer\\n\\n# To define maximum number of columns to be displayed in a dataframe\\npd.set_option(\\\"display.max_columns\\\", None)\\n\\n# To supress scientific notations for a dataframe\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.3f\\\" % x)\\n\\n# To supress warnings\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# This will help in making the Python code more structured automatically (good coding practice)\\n#%load_ext nb_black\";\n                var nbb_formatted_code = \"# To help with reading and manipulating data\\nimport pandas as pd\\nimport numpy as np\\n\\n# To help with data visualization\\n%matplotlib inline\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# To be used for missing value imputation\\nfrom sklearn.impute import SimpleImputer\\n\\n# To help with model building\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.ensemble import (\\n    AdaBoostClassifier,\\n    GradientBoostingClassifier,\\n    RandomForestClassifier,\\n    BaggingClassifier,\\n)\\nfrom xgboost import XGBClassifier\\n\\n# To get different metric scores, and split data\\nfrom sklearn import metrics\\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\\nfrom sklearn.metrics import (\\n    f1_score,\\n    accuracy_score,\\n    recall_score,\\n    precision_score,\\n    confusion_matrix,\\n    roc_auc_score,\\n    # plot_confusion_matrix,\\n    ConfusionMatrixDisplay,\\n)\\n\\n# To be used for data scaling and one hot encoding\\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\\n\\n# To be used for tuning the model\\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\\n\\n# To be used for creating pipelines and personalizing them\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.compose import ColumnTransformer\\n\\n# To define maximum number of columns to be displayed in a dataframe\\npd.set_option(\\\"display.max_columns\\\", None)\\n\\n# To supress scientific notations for a dataframe\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.3f\\\" % x)\\n\\n# To supress warnings\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# This will help in making the Python code more structured automatically (good coding practice)\\n# %load_ext nb_black\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 80;\n                var nbb_unformatted_code = \"# To help with reading and manipulating data\\nimport pandas as pd\\nimport numpy as np\\n\\n# To help with data visualization\\n%matplotlib inline\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# To be used for missing value imputation\\nfrom sklearn.impute import SimpleImputer\\n\\n# To help with model building\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.ensemble import (\\n    AdaBoostClassifier,\\n    GradientBoostingClassifier,\\n    RandomForestClassifier,\\n    BaggingClassifier,\\n)\\nfrom xgboost import XGBClassifier\\n\\n# To get different metric scores, and split data\\nfrom sklearn import metrics\\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\\nfrom sklearn.metrics import (\\n    f1_score,\\n    accuracy_score,\\n    recall_score,\\n    precision_score,\\n    confusion_matrix,\\n    roc_auc_score,\\n    # plot_confusion_matrix,\\n    ConfusionMatrixDisplay,\\n)\\n\\n# To be used for data scaling and one hot encoding\\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\\n\\n# To be used for tuning the model\\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\\n\\n# To be used for creating pipelines and personalizing them\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.compose import ColumnTransformer\\n\\n# To define maximum number of columns to be displayed in a dataframe\\npd.set_option(\\\"display.max_columns\\\", None)\\n\\n# To supress scientific notations for a dataframe\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.3f\\\" % x)\\n\\n# To supress warnings\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# This will help in making the Python code more structured automatically (good coding practice)\\n#%load_ext nb_black\";\n                var nbb_formatted_code = \"# To help with reading and manipulating data\\nimport pandas as pd\\nimport numpy as np\\n\\n# To help with data visualization\\n%matplotlib inline\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# To be used for missing value imputation\\nfrom sklearn.impute import SimpleImputer\\n\\n# To help with model building\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.ensemble import (\\n    AdaBoostClassifier,\\n    GradientBoostingClassifier,\\n    RandomForestClassifier,\\n    BaggingClassifier,\\n)\\nfrom xgboost import XGBClassifier\\n\\n# To get different metric scores, and split data\\nfrom sklearn import metrics\\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\\nfrom sklearn.metrics import (\\n    f1_score,\\n    accuracy_score,\\n    recall_score,\\n    precision_score,\\n    confusion_matrix,\\n    roc_auc_score,\\n    # plot_confusion_matrix,\\n    ConfusionMatrixDisplay,\\n)\\n\\n# To be used for data scaling and one hot encoding\\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\\n\\n# To be used for tuning the model\\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\\n\\n# To be used for creating pipelines and personalizing them\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.compose import ColumnTransformer\\n\\n# To define maximum number of columns to be displayed in a dataframe\\npd.set_option(\\\"display.max_columns\\\", None)\\n\\n# To supress scientific notations for a dataframe\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.3f\\\" % x)\\n\\n# To supress warnings\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# This will help in making the Python code more structured automatically (good coding practice)\\n# %load_ext nb_black\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# To help with reading and manipulating data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# To help with data visualization\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# To be used for missing value imputation\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# To help with model building\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import (\n",
        "    AdaBoostClassifier,\n",
        "    GradientBoostingClassifier,\n",
        "    RandomForestClassifier,\n",
        "    BaggingClassifier,\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# To get different metric scores, and split data\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    accuracy_score,\n",
        "    recall_score,\n",
        "    precision_score,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    # plot_confusion_matrix,\n",
        "    ConfusionMatrixDisplay,\n",
        ")\n",
        "\n",
        "# To be used for data scaling and one hot encoding\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
        "\n",
        "# To be used for tuning the model\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "# To be used for creating pipelines and personalizing them\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# To define maximum number of columns to be displayed in a dataframe\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "\n",
        "# To supress scientific notations for a dataframe\n",
        "pd.set_option(\"display.float_format\", lambda x: \"%.3f\" % x)\n",
        "\n",
        "# To supress warnings\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# This will help in making the Python code more structured automatically (good coding practice)\n",
        "#%load_ext nb_black"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The nb_black extension is already loaded. To reload it, use:\n",
            "  %reload_ext nb_black\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 81;\n                var nbb_unformatted_code = \"# legacy: this will help in making the Python code more structured automatically (help adhere to good coding practices)\\n%load_ext nb_black\\n\\n\\nimport pandas as pd\\n# import numpy as np\\nfrom sklearn import metrics\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\nimport warnings\\nwarnings.filterwarnings('ignore')\\nimport seaborn as sns\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.model_selection import GridSearchCV # (To tune different models)\\nfrom sklearn import metrics # new\\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier # new\\nfrom sklearn.linear_model import LogisticRegression # new\\n\\n# legacy import:\\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\\n\\n# On:\\n# previous:\\n# Removes the limit for the number of displayed columns\\npd.set_option(\\\"display.max_columns\\\", None)\\n# Sets the limit for the number of displayed rows\\n# pd.set_option(\\\"display.max_rows\\\", 200)\\n# setting the precision of floating numbers to 5 decimal points\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.5f\\\" % x)\\n# end previous\\n\\n# To build model for prediction\\nimport statsmodels.stats.api as sms\\n\\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\\nimport statsmodels.api as sm\\nfrom statsmodels.tools.tools import add_constant\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn import tree\\n\\n# To get diferent metric scores\\n# from previous\\n# duplicate import from above\\nfrom sklearn.metrics import (\\n    f1_score,\\n    accuracy_score,\\n    recall_score,\\n    precision_score,\\n    confusion_matrix,\\n    roc_auc_score,\\n    ConfusionMatrixDisplay,\\n    precision_recall_curve,\\n    roc_curve,\\n    make_scorer,\\n)\\n\\n%reload_ext nb_black\";\n                var nbb_formatted_code = \"# legacy: this will help in making the Python code more structured automatically (help adhere to good coding practices)\\n%load_ext nb_black\\n\\n\\nimport pandas as pd\\n\\n# import numpy as np\\nfrom sklearn import metrics\\nimport matplotlib.pyplot as plt\\n\\n%matplotlib inline\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\nimport seaborn as sns\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.model_selection import GridSearchCV  # (To tune different models)\\nfrom sklearn import metrics  # new\\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier  # new\\nfrom sklearn.linear_model import LogisticRegression  # new\\n\\n# legacy import:\\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\\n\\n# On:\\n# previous:\\n# Removes the limit for the number of displayed columns\\npd.set_option(\\\"display.max_columns\\\", None)\\n# Sets the limit for the number of displayed rows\\n# pd.set_option(\\\"display.max_rows\\\", 200)\\n# setting the precision of floating numbers to 5 decimal points\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.5f\\\" % x)\\n# end previous\\n\\n# To build model for prediction\\nimport statsmodels.stats.api as sms\\n\\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\\nimport statsmodels.api as sm\\nfrom statsmodels.tools.tools import add_constant\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn import tree\\n\\n# To get diferent metric scores\\n# from previous\\n# duplicate import from above\\nfrom sklearn.metrics import (\\n    f1_score,\\n    accuracy_score,\\n    recall_score,\\n    precision_score,\\n    confusion_matrix,\\n    roc_auc_score,\\n    ConfusionMatrixDisplay,\\n    precision_recall_curve,\\n    roc_curve,\\n    make_scorer,\\n)\\n\\n%reload_ext nb_black\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 81;\n                var nbb_unformatted_code = \"# legacy: this will help in making the Python code more structured automatically (help adhere to good coding practices)\\n%load_ext nb_black\\n\\n\\nimport pandas as pd\\n# import numpy as np\\nfrom sklearn import metrics\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\nimport warnings\\nwarnings.filterwarnings('ignore')\\nimport seaborn as sns\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.model_selection import GridSearchCV # (To tune different models)\\nfrom sklearn import metrics # new\\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier # new\\nfrom sklearn.linear_model import LogisticRegression # new\\n\\n# legacy import:\\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\\n\\n# On:\\n# previous:\\n# Removes the limit for the number of displayed columns\\npd.set_option(\\\"display.max_columns\\\", None)\\n# Sets the limit for the number of displayed rows\\n# pd.set_option(\\\"display.max_rows\\\", 200)\\n# setting the precision of floating numbers to 5 decimal points\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.5f\\\" % x)\\n# end previous\\n\\n# To build model for prediction\\nimport statsmodels.stats.api as sms\\n\\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\\nimport statsmodels.api as sm\\nfrom statsmodels.tools.tools import add_constant\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn import tree\\n\\n# To get diferent metric scores\\n# from previous\\n# duplicate import from above\\nfrom sklearn.metrics import (\\n    f1_score,\\n    accuracy_score,\\n    recall_score,\\n    precision_score,\\n    confusion_matrix,\\n    roc_auc_score,\\n    ConfusionMatrixDisplay,\\n    precision_recall_curve,\\n    roc_curve,\\n    make_scorer,\\n)\\n\\n%reload_ext nb_black\";\n                var nbb_formatted_code = \"# legacy: this will help in making the Python code more structured automatically (help adhere to good coding practices)\\n%load_ext nb_black\\n\\n\\nimport pandas as pd\\n\\n# import numpy as np\\nfrom sklearn import metrics\\nimport matplotlib.pyplot as plt\\n\\n%matplotlib inline\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\nimport seaborn as sns\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.model_selection import GridSearchCV  # (To tune different models)\\nfrom sklearn import metrics  # new\\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier  # new\\nfrom sklearn.linear_model import LogisticRegression  # new\\n\\n# legacy import:\\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\\n\\n# On:\\n# previous:\\n# Removes the limit for the number of displayed columns\\npd.set_option(\\\"display.max_columns\\\", None)\\n# Sets the limit for the number of displayed rows\\n# pd.set_option(\\\"display.max_rows\\\", 200)\\n# setting the precision of floating numbers to 5 decimal points\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.5f\\\" % x)\\n# end previous\\n\\n# To build model for prediction\\nimport statsmodels.stats.api as sms\\n\\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\\nimport statsmodels.api as sm\\nfrom statsmodels.tools.tools import add_constant\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn import tree\\n\\n# To get diferent metric scores\\n# from previous\\n# duplicate import from above\\nfrom sklearn.metrics import (\\n    f1_score,\\n    accuracy_score,\\n    recall_score,\\n    precision_score,\\n    confusion_matrix,\\n    roc_auc_score,\\n    ConfusionMatrixDisplay,\\n    precision_recall_curve,\\n    roc_curve,\\n    make_scorer,\\n)\\n\\n%reload_ext nb_black\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 81;\n                var nbb_unformatted_code = \"# legacy: this will help in making the Python code more structured automatically (help adhere to good coding practices)\\n%load_ext nb_black\\n\\n\\nimport pandas as pd\\n# import numpy as np\\nfrom sklearn import metrics\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\nimport warnings\\nwarnings.filterwarnings('ignore')\\nimport seaborn as sns\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.model_selection import GridSearchCV # (To tune different models)\\nfrom sklearn import metrics # new\\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier # new\\nfrom sklearn.linear_model import LogisticRegression # new\\n\\n# legacy import:\\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\\n\\n# On:\\n# previous:\\n# Removes the limit for the number of displayed columns\\npd.set_option(\\\"display.max_columns\\\", None)\\n# Sets the limit for the number of displayed rows\\n# pd.set_option(\\\"display.max_rows\\\", 200)\\n# setting the precision of floating numbers to 5 decimal points\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.5f\\\" % x)\\n# end previous\\n\\n# To build model for prediction\\nimport statsmodels.stats.api as sms\\n\\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\\nimport statsmodels.api as sm\\nfrom statsmodels.tools.tools import add_constant\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn import tree\\n\\n# To get diferent metric scores\\n# from previous\\n# duplicate import from above\\nfrom sklearn.metrics import (\\n    f1_score,\\n    accuracy_score,\\n    recall_score,\\n    precision_score,\\n    confusion_matrix,\\n    roc_auc_score,\\n    ConfusionMatrixDisplay,\\n    precision_recall_curve,\\n    roc_curve,\\n    make_scorer,\\n)\\n\\n%reload_ext nb_black\";\n                var nbb_formatted_code = \"# legacy: this will help in making the Python code more structured automatically (help adhere to good coding practices)\\n%load_ext nb_black\\n\\n\\nimport pandas as pd\\n\\n# import numpy as np\\nfrom sklearn import metrics\\nimport matplotlib.pyplot as plt\\n\\n%matplotlib inline\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\nimport seaborn as sns\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.model_selection import GridSearchCV  # (To tune different models)\\nfrom sklearn import metrics  # new\\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier  # new\\nfrom sklearn.linear_model import LogisticRegression  # new\\n\\n# legacy import:\\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\\n\\n# On:\\n# previous:\\n# Removes the limit for the number of displayed columns\\npd.set_option(\\\"display.max_columns\\\", None)\\n# Sets the limit for the number of displayed rows\\n# pd.set_option(\\\"display.max_rows\\\", 200)\\n# setting the precision of floating numbers to 5 decimal points\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.5f\\\" % x)\\n# end previous\\n\\n# To build model for prediction\\nimport statsmodels.stats.api as sms\\n\\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\\nimport statsmodels.api as sm\\nfrom statsmodels.tools.tools import add_constant\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn import tree\\n\\n# To get diferent metric scores\\n# from previous\\n# duplicate import from above\\nfrom sklearn.metrics import (\\n    f1_score,\\n    accuracy_score,\\n    recall_score,\\n    precision_score,\\n    confusion_matrix,\\n    roc_auc_score,\\n    ConfusionMatrixDisplay,\\n    precision_recall_curve,\\n    roc_curve,\\n    make_scorer,\\n)\\n\\n%reload_ext nb_black\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 81;\n                var nbb_unformatted_code = \"# legacy: this will help in making the Python code more structured automatically (help adhere to good coding practices)\\n%load_ext nb_black\\n\\n\\nimport pandas as pd\\n# import numpy as np\\nfrom sklearn import metrics\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\nimport warnings\\nwarnings.filterwarnings('ignore')\\nimport seaborn as sns\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.model_selection import GridSearchCV # (To tune different models)\\nfrom sklearn import metrics # new\\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier # new\\nfrom sklearn.linear_model import LogisticRegression # new\\n\\n# legacy import:\\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\\n\\n# On:\\n# previous:\\n# Removes the limit for the number of displayed columns\\npd.set_option(\\\"display.max_columns\\\", None)\\n# Sets the limit for the number of displayed rows\\n# pd.set_option(\\\"display.max_rows\\\", 200)\\n# setting the precision of floating numbers to 5 decimal points\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.5f\\\" % x)\\n# end previous\\n\\n# To build model for prediction\\nimport statsmodels.stats.api as sms\\n\\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\\nimport statsmodels.api as sm\\nfrom statsmodels.tools.tools import add_constant\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn import tree\\n\\n# To get diferent metric scores\\n# from previous\\n# duplicate import from above\\nfrom sklearn.metrics import (\\n    f1_score,\\n    accuracy_score,\\n    recall_score,\\n    precision_score,\\n    confusion_matrix,\\n    roc_auc_score,\\n    ConfusionMatrixDisplay,\\n    precision_recall_curve,\\n    roc_curve,\\n    make_scorer,\\n)\\n\\n%reload_ext nb_black\";\n                var nbb_formatted_code = \"# legacy: this will help in making the Python code more structured automatically (help adhere to good coding practices)\\n%load_ext nb_black\\n\\n\\nimport pandas as pd\\n\\n# import numpy as np\\nfrom sklearn import metrics\\nimport matplotlib.pyplot as plt\\n\\n%matplotlib inline\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\nimport seaborn as sns\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.model_selection import GridSearchCV  # (To tune different models)\\nfrom sklearn import metrics  # new\\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier  # new\\nfrom sklearn.linear_model import LogisticRegression  # new\\n\\n# legacy import:\\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning\\n\\n# On:\\n# previous:\\n# Removes the limit for the number of displayed columns\\npd.set_option(\\\"display.max_columns\\\", None)\\n# Sets the limit for the number of displayed rows\\n# pd.set_option(\\\"display.max_rows\\\", 200)\\n# setting the precision of floating numbers to 5 decimal points\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.5f\\\" % x)\\n# end previous\\n\\n# To build model for prediction\\nimport statsmodels.stats.api as sms\\n\\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\\nimport statsmodels.api as sm\\nfrom statsmodels.tools.tools import add_constant\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn import tree\\n\\n# To get diferent metric scores\\n# from previous\\n# duplicate import from above\\nfrom sklearn.metrics import (\\n    f1_score,\\n    accuracy_score,\\n    recall_score,\\n    precision_score,\\n    confusion_matrix,\\n    roc_auc_score,\\n    ConfusionMatrixDisplay,\\n    precision_recall_curve,\\n    roc_curve,\\n    make_scorer,\\n)\\n\\n%reload_ext nb_black\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# legacy: this will help in making the Python code more structured automatically (help adhere to good coding practices)\n",
        "%load_ext nb_black\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "# import numpy as np\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV # (To tune different models)\n",
        "from sklearn import metrics # new\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier # new\n",
        "from sklearn.linear_model import LogisticRegression # new\n",
        "\n",
        "# legacy import:\n",
        "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
        "\n",
        "# On:\n",
        "# previous:\n",
        "# Removes the limit for the number of displayed columns\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "# Sets the limit for the number of displayed rows\n",
        "# pd.set_option(\"display.max_rows\", 200)\n",
        "# setting the precision of floating numbers to 5 decimal points\n",
        "pd.set_option(\"display.float_format\", lambda x: \"%.5f\" % x)\n",
        "# end previous\n",
        "\n",
        "# To build model for prediction\n",
        "import statsmodels.stats.api as sms\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tools.tools import add_constant\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import tree\n",
        "\n",
        "# To get diferent metric scores\n",
        "# from previous\n",
        "# duplicate import from above\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    accuracy_score,\n",
        "    recall_score,\n",
        "    precision_score,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    ConfusionMatrixDisplay,\n",
        "    precision_recall_curve,\n",
        "    roc_curve,\n",
        "    make_scorer,\n",
        ")\n",
        "\n",
        "%reload_ext nb_black\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 82;\n                var nbb_unformatted_code = \"# add matplotlib tutorial\\n# https://matplotlib.org/stable/tutorials/introductory/pyplot.html\\nimport matplotlib.pyplot as plt\\nplt.plot([1, 2, 3, 4])\\nplt.ylabel('some numbers')\\nplt.show()\";\n                var nbb_formatted_code = \"# add matplotlib tutorial\\n# https://matplotlib.org/stable/tutorials/introductory/pyplot.html\\nimport matplotlib.pyplot as plt\\n\\nplt.plot([1, 2, 3, 4])\\nplt.ylabel(\\\"some numbers\\\")\\nplt.show()\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 82;\n                var nbb_unformatted_code = \"# add matplotlib tutorial\\n# https://matplotlib.org/stable/tutorials/introductory/pyplot.html\\nimport matplotlib.pyplot as plt\\nplt.plot([1, 2, 3, 4])\\nplt.ylabel('some numbers')\\nplt.show()\";\n                var nbb_formatted_code = \"# add matplotlib tutorial\\n# https://matplotlib.org/stable/tutorials/introductory/pyplot.html\\nimport matplotlib.pyplot as plt\\n\\nplt.plot([1, 2, 3, 4])\\nplt.ylabel(\\\"some numbers\\\")\\nplt.show()\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 82;\n                var nbb_unformatted_code = \"# add matplotlib tutorial\\n# https://matplotlib.org/stable/tutorials/introductory/pyplot.html\\nimport matplotlib.pyplot as plt\\nplt.plot([1, 2, 3, 4])\\nplt.ylabel('some numbers')\\nplt.show()\";\n                var nbb_formatted_code = \"# add matplotlib tutorial\\n# https://matplotlib.org/stable/tutorials/introductory/pyplot.html\\nimport matplotlib.pyplot as plt\\n\\nplt.plot([1, 2, 3, 4])\\nplt.ylabel(\\\"some numbers\\\")\\nplt.show()\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 82;\n                var nbb_unformatted_code = \"# add matplotlib tutorial\\n# https://matplotlib.org/stable/tutorials/introductory/pyplot.html\\nimport matplotlib.pyplot as plt\\nplt.plot([1, 2, 3, 4])\\nplt.ylabel('some numbers')\\nplt.show()\";\n                var nbb_formatted_code = \"# add matplotlib tutorial\\n# https://matplotlib.org/stable/tutorials/introductory/pyplot.html\\nimport matplotlib.pyplot as plt\\n\\nplt.plot([1, 2, 3, 4])\\nplt.ylabel(\\\"some numbers\\\")\\nplt.show()\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# add matplotlib tutorial\n",
        "# https://matplotlib.org/stable/tutorials/introductory/pyplot.html\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot([1, 2, 3, 4])\n",
        "plt.ylabel('some numbers')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f4771e9e680>]"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 83;\n                var nbb_unformatted_code = \"plt.plot([1, 2, 3, 4], [1, 4, 9, 16])\";\n                var nbb_formatted_code = \"plt.plot([1, 2, 3, 4], [1, 4, 9, 16])\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 83;\n                var nbb_unformatted_code = \"plt.plot([1, 2, 3, 4], [1, 4, 9, 16])\";\n                var nbb_formatted_code = \"plt.plot([1, 2, 3, 4], [1, 4, 9, 16])\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 83;\n                var nbb_unformatted_code = \"plt.plot([1, 2, 3, 4], [1, 4, 9, 16])\";\n                var nbb_formatted_code = \"plt.plot([1, 2, 3, 4], [1, 4, 9, 16])\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 83;\n                var nbb_unformatted_code = \"plt.plot([1, 2, 3, 4], [1, 4, 9, 16])\";\n                var nbb_formatted_code = \"plt.plot([1, 2, 3, 4], [1, 4, 9, 16])\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot([1, 2, 3, 4], [1, 4, 9, 16])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 84;\n                var nbb_unformatted_code = \"import warnings\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# import numpy as np   \\n# import pandas as pd\\n\\n# import seaborn as sns\\n# import matplotlib.pyplot as plt\\n\\nfrom sklearn.tree import DecisionTreeRegressor\\n# from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor\\n# from xgboost import XGBRegressor\\n# from sklearn import metrics\\nfrom sklearn.model_selection import GridSearchCV, train_test_split\";\n                var nbb_formatted_code = \"import warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# import numpy as np\\n# import pandas as pd\\n\\n# import seaborn as sns\\n# import matplotlib.pyplot as plt\\n\\nfrom sklearn.tree import DecisionTreeRegressor\\n\\n# from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor\\n# from xgboost import XGBRegressor\\n# from sklearn import metrics\\nfrom sklearn.model_selection import GridSearchCV, train_test_split\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 84;\n                var nbb_unformatted_code = \"import warnings\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# import numpy as np   \\n# import pandas as pd\\n\\n# import seaborn as sns\\n# import matplotlib.pyplot as plt\\n\\nfrom sklearn.tree import DecisionTreeRegressor\\n# from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor\\n# from xgboost import XGBRegressor\\n# from sklearn import metrics\\nfrom sklearn.model_selection import GridSearchCV, train_test_split\";\n                var nbb_formatted_code = \"import warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# import numpy as np\\n# import pandas as pd\\n\\n# import seaborn as sns\\n# import matplotlib.pyplot as plt\\n\\nfrom sklearn.tree import DecisionTreeRegressor\\n\\n# from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor\\n# from xgboost import XGBRegressor\\n# from sklearn import metrics\\nfrom sklearn.model_selection import GridSearchCV, train_test_split\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 84;\n                var nbb_unformatted_code = \"import warnings\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# import numpy as np   \\n# import pandas as pd\\n\\n# import seaborn as sns\\n# import matplotlib.pyplot as plt\\n\\nfrom sklearn.tree import DecisionTreeRegressor\\n# from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor\\n# from xgboost import XGBRegressor\\n# from sklearn import metrics\\nfrom sklearn.model_selection import GridSearchCV, train_test_split\";\n                var nbb_formatted_code = \"import warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# import numpy as np\\n# import pandas as pd\\n\\n# import seaborn as sns\\n# import matplotlib.pyplot as plt\\n\\nfrom sklearn.tree import DecisionTreeRegressor\\n\\n# from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor\\n# from xgboost import XGBRegressor\\n# from sklearn import metrics\\nfrom sklearn.model_selection import GridSearchCV, train_test_split\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 84;\n                var nbb_unformatted_code = \"import warnings\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# import numpy as np   \\n# import pandas as pd\\n\\n# import seaborn as sns\\n# import matplotlib.pyplot as plt\\n\\nfrom sklearn.tree import DecisionTreeRegressor\\n# from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor\\n# from xgboost import XGBRegressor\\n# from sklearn import metrics\\nfrom sklearn.model_selection import GridSearchCV, train_test_split\";\n                var nbb_formatted_code = \"import warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# import numpy as np\\n# import pandas as pd\\n\\n# import seaborn as sns\\n# import matplotlib.pyplot as plt\\n\\nfrom sklearn.tree import DecisionTreeRegressor\\n\\n# from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor\\n# from xgboost import XGBRegressor\\n# from sklearn import metrics\\nfrom sklearn.model_selection import GridSearchCV, train_test_split\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# import numpy as np   \n",
        "# import pandas as pd\n",
        "\n",
        "# import seaborn as sns\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "# from sklearn.ensemble import BaggingRegressor,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor\n",
        "# from xgboost import XGBRegressor\n",
        "# from sklearn import metrics\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The nb_black extension is already loaded. To reload it, use:\n",
            "  %reload_ext nb_black\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 85;\n                var nbb_unformatted_code = \"# this will help in making the Python code more structured automatically (good coding practice)\\n%load_ext nb_black\\n\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# Libraries to help with reading and manipulating data\\nimport numpy as np\\nimport pandas as pd\\n\\n# Library to split data\\nfrom sklearn.model_selection import train_test_split\\n\\n# libaries to help with data visualization\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# On\\n# Removes the limit for the number of displayed columns\\n# pd.set_option(\\\"display.max_columns\\\", None)\\n# Sets the limit for the number of displayed rows\\n# pd.set_option(\\\"display.max_rows\\\", 100)\\n\\n\\n# Libraries different ensemble classifiers\\nfrom sklearn.ensemble import (\\n    BaggingClassifier,\\n    RandomForestClassifier,\\n    AdaBoostClassifier,\\n    GradientBoostingClassifier,\\n    StackingClassifier,\\n)\\n\\nfrom xgboost import XGBClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\n# Libraries to get different metric scores\\nfrom sklearn import metrics\\nfrom sklearn.metrics import (\\n    confusion_matrix,\\n    accuracy_score,\\n    precision_score,\\n    recall_score,\\n    f1_score,\\n)\\n\\n# To tune different models\\nfrom sklearn.model_selection import GridSearchCV\";\n                var nbb_formatted_code = \"# this will help in making the Python code more structured automatically (good coding practice)\\n%load_ext nb_black\\n\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# Libraries to help with reading and manipulating data\\nimport numpy as np\\nimport pandas as pd\\n\\n# Library to split data\\nfrom sklearn.model_selection import train_test_split\\n\\n# libaries to help with data visualization\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# On\\n# Removes the limit for the number of displayed columns\\n# pd.set_option(\\\"display.max_columns\\\", None)\\n# Sets the limit for the number of displayed rows\\n# pd.set_option(\\\"display.max_rows\\\", 100)\\n\\n\\n# Libraries different ensemble classifiers\\nfrom sklearn.ensemble import (\\n    BaggingClassifier,\\n    RandomForestClassifier,\\n    AdaBoostClassifier,\\n    GradientBoostingClassifier,\\n    StackingClassifier,\\n)\\n\\nfrom xgboost import XGBClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\n# Libraries to get different metric scores\\nfrom sklearn import metrics\\nfrom sklearn.metrics import (\\n    confusion_matrix,\\n    accuracy_score,\\n    precision_score,\\n    recall_score,\\n    f1_score,\\n)\\n\\n# To tune different models\\nfrom sklearn.model_selection import GridSearchCV\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 85;\n                var nbb_unformatted_code = \"# this will help in making the Python code more structured automatically (good coding practice)\\n%load_ext nb_black\\n\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# Libraries to help with reading and manipulating data\\nimport numpy as np\\nimport pandas as pd\\n\\n# Library to split data\\nfrom sklearn.model_selection import train_test_split\\n\\n# libaries to help with data visualization\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# On\\n# Removes the limit for the number of displayed columns\\n# pd.set_option(\\\"display.max_columns\\\", None)\\n# Sets the limit for the number of displayed rows\\n# pd.set_option(\\\"display.max_rows\\\", 100)\\n\\n\\n# Libraries different ensemble classifiers\\nfrom sklearn.ensemble import (\\n    BaggingClassifier,\\n    RandomForestClassifier,\\n    AdaBoostClassifier,\\n    GradientBoostingClassifier,\\n    StackingClassifier,\\n)\\n\\nfrom xgboost import XGBClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\n# Libraries to get different metric scores\\nfrom sklearn import metrics\\nfrom sklearn.metrics import (\\n    confusion_matrix,\\n    accuracy_score,\\n    precision_score,\\n    recall_score,\\n    f1_score,\\n)\\n\\n# To tune different models\\nfrom sklearn.model_selection import GridSearchCV\";\n                var nbb_formatted_code = \"# this will help in making the Python code more structured automatically (good coding practice)\\n%load_ext nb_black\\n\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# Libraries to help with reading and manipulating data\\nimport numpy as np\\nimport pandas as pd\\n\\n# Library to split data\\nfrom sklearn.model_selection import train_test_split\\n\\n# libaries to help with data visualization\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# On\\n# Removes the limit for the number of displayed columns\\n# pd.set_option(\\\"display.max_columns\\\", None)\\n# Sets the limit for the number of displayed rows\\n# pd.set_option(\\\"display.max_rows\\\", 100)\\n\\n\\n# Libraries different ensemble classifiers\\nfrom sklearn.ensemble import (\\n    BaggingClassifier,\\n    RandomForestClassifier,\\n    AdaBoostClassifier,\\n    GradientBoostingClassifier,\\n    StackingClassifier,\\n)\\n\\nfrom xgboost import XGBClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\n# Libraries to get different metric scores\\nfrom sklearn import metrics\\nfrom sklearn.metrics import (\\n    confusion_matrix,\\n    accuracy_score,\\n    precision_score,\\n    recall_score,\\n    f1_score,\\n)\\n\\n# To tune different models\\nfrom sklearn.model_selection import GridSearchCV\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 85;\n                var nbb_unformatted_code = \"# this will help in making the Python code more structured automatically (good coding practice)\\n%load_ext nb_black\\n\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# Libraries to help with reading and manipulating data\\nimport numpy as np\\nimport pandas as pd\\n\\n# Library to split data\\nfrom sklearn.model_selection import train_test_split\\n\\n# libaries to help with data visualization\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# On\\n# Removes the limit for the number of displayed columns\\n# pd.set_option(\\\"display.max_columns\\\", None)\\n# Sets the limit for the number of displayed rows\\n# pd.set_option(\\\"display.max_rows\\\", 100)\\n\\n\\n# Libraries different ensemble classifiers\\nfrom sklearn.ensemble import (\\n    BaggingClassifier,\\n    RandomForestClassifier,\\n    AdaBoostClassifier,\\n    GradientBoostingClassifier,\\n    StackingClassifier,\\n)\\n\\nfrom xgboost import XGBClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\n# Libraries to get different metric scores\\nfrom sklearn import metrics\\nfrom sklearn.metrics import (\\n    confusion_matrix,\\n    accuracy_score,\\n    precision_score,\\n    recall_score,\\n    f1_score,\\n)\\n\\n# To tune different models\\nfrom sklearn.model_selection import GridSearchCV\";\n                var nbb_formatted_code = \"# this will help in making the Python code more structured automatically (good coding practice)\\n%load_ext nb_black\\n\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# Libraries to help with reading and manipulating data\\nimport numpy as np\\nimport pandas as pd\\n\\n# Library to split data\\nfrom sklearn.model_selection import train_test_split\\n\\n# libaries to help with data visualization\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# On\\n# Removes the limit for the number of displayed columns\\n# pd.set_option(\\\"display.max_columns\\\", None)\\n# Sets the limit for the number of displayed rows\\n# pd.set_option(\\\"display.max_rows\\\", 100)\\n\\n\\n# Libraries different ensemble classifiers\\nfrom sklearn.ensemble import (\\n    BaggingClassifier,\\n    RandomForestClassifier,\\n    AdaBoostClassifier,\\n    GradientBoostingClassifier,\\n    StackingClassifier,\\n)\\n\\nfrom xgboost import XGBClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\n# Libraries to get different metric scores\\nfrom sklearn import metrics\\nfrom sklearn.metrics import (\\n    confusion_matrix,\\n    accuracy_score,\\n    precision_score,\\n    recall_score,\\n    f1_score,\\n)\\n\\n# To tune different models\\nfrom sklearn.model_selection import GridSearchCV\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 85;\n                var nbb_unformatted_code = \"# this will help in making the Python code more structured automatically (good coding practice)\\n%load_ext nb_black\\n\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# Libraries to help with reading and manipulating data\\nimport numpy as np\\nimport pandas as pd\\n\\n# Library to split data\\nfrom sklearn.model_selection import train_test_split\\n\\n# libaries to help with data visualization\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# On\\n# Removes the limit for the number of displayed columns\\n# pd.set_option(\\\"display.max_columns\\\", None)\\n# Sets the limit for the number of displayed rows\\n# pd.set_option(\\\"display.max_rows\\\", 100)\\n\\n\\n# Libraries different ensemble classifiers\\nfrom sklearn.ensemble import (\\n    BaggingClassifier,\\n    RandomForestClassifier,\\n    AdaBoostClassifier,\\n    GradientBoostingClassifier,\\n    StackingClassifier,\\n)\\n\\nfrom xgboost import XGBClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\n# Libraries to get different metric scores\\nfrom sklearn import metrics\\nfrom sklearn.metrics import (\\n    confusion_matrix,\\n    accuracy_score,\\n    precision_score,\\n    recall_score,\\n    f1_score,\\n)\\n\\n# To tune different models\\nfrom sklearn.model_selection import GridSearchCV\";\n                var nbb_formatted_code = \"# this will help in making the Python code more structured automatically (good coding practice)\\n%load_ext nb_black\\n\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# Libraries to help with reading and manipulating data\\nimport numpy as np\\nimport pandas as pd\\n\\n# Library to split data\\nfrom sklearn.model_selection import train_test_split\\n\\n# libaries to help with data visualization\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# On\\n# Removes the limit for the number of displayed columns\\n# pd.set_option(\\\"display.max_columns\\\", None)\\n# Sets the limit for the number of displayed rows\\n# pd.set_option(\\\"display.max_rows\\\", 100)\\n\\n\\n# Libraries different ensemble classifiers\\nfrom sklearn.ensemble import (\\n    BaggingClassifier,\\n    RandomForestClassifier,\\n    AdaBoostClassifier,\\n    GradientBoostingClassifier,\\n    StackingClassifier,\\n)\\n\\nfrom xgboost import XGBClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\n# Libraries to get different metric scores\\nfrom sklearn import metrics\\nfrom sklearn.metrics import (\\n    confusion_matrix,\\n    accuracy_score,\\n    precision_score,\\n    recall_score,\\n    f1_score,\\n)\\n\\n# To tune different models\\nfrom sklearn.model_selection import GridSearchCV\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# this will help in making the Python code more structured automatically (good coding practice)\n",
        "%load_ext nb_black\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Libraries to help with reading and manipulating data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Library to split data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# libaries to help with data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# On\n",
        "# Removes the limit for the number of displayed columns\n",
        "# pd.set_option(\"display.max_columns\", None)\n",
        "# Sets the limit for the number of displayed rows\n",
        "# pd.set_option(\"display.max_rows\", 100)\n",
        "\n",
        "\n",
        "# Libraries different ensemble classifiers\n",
        "from sklearn.ensemble import (\n",
        "    BaggingClassifier,\n",
        "    RandomForestClassifier,\n",
        "    AdaBoostClassifier,\n",
        "    GradientBoostingClassifier,\n",
        "    StackingClassifier,\n",
        ")\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Libraries to get different metric scores\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        ")\n",
        "\n",
        "# To tune different models\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 86;\n                var nbb_unformatted_code = \"# has SMOTE\\n# To help with reading and manipulation of data\\nimport numpy as np\\nfrom numpy import array\\nimport pandas as pd\\n\\n# To help with data visualization\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# To split the data\\nfrom sklearn.model_selection import train_test_split\\n\\n# To impute missing values\\nfrom sklearn.impute import SimpleImputer\\n\\n# To do one-hot encoding\\nfrom sklearn.preprocessing import OneHotEncoder\\n\\n# To build a decision tree model\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\n# To get different performance metrics\\nimport sklearn.metrics as metrics\\nfrom sklearn.metrics import (\\n    classification_report,\\n    confusion_matrix,\\n    recall_score,\\n    accuracy_score,\\n    precision_score,\\n    f1_score,\\n)\\n\\n\\n# To undersample and oversample the data\\nfrom imblearn.over_sampling import SMOTE\\nfrom imblearn.under_sampling import RandomUnderSampler\\n\\n# To suppress warnings\\nimport warnings\\nwarnings.filterwarnings(\\\"ignore\\\")\";\n                var nbb_formatted_code = \"# has SMOTE\\n# To help with reading and manipulation of data\\nimport numpy as np\\nfrom numpy import array\\nimport pandas as pd\\n\\n# To help with data visualization\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# To split the data\\nfrom sklearn.model_selection import train_test_split\\n\\n# To impute missing values\\nfrom sklearn.impute import SimpleImputer\\n\\n# To do one-hot encoding\\nfrom sklearn.preprocessing import OneHotEncoder\\n\\n# To build a decision tree model\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\n# To get different performance metrics\\nimport sklearn.metrics as metrics\\nfrom sklearn.metrics import (\\n    classification_report,\\n    confusion_matrix,\\n    recall_score,\\n    accuracy_score,\\n    precision_score,\\n    f1_score,\\n)\\n\\n\\n# To undersample and oversample the data\\nfrom imblearn.over_sampling import SMOTE\\nfrom imblearn.under_sampling import RandomUnderSampler\\n\\n# To suppress warnings\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 86;\n                var nbb_unformatted_code = \"# has SMOTE\\n# To help with reading and manipulation of data\\nimport numpy as np\\nfrom numpy import array\\nimport pandas as pd\\n\\n# To help with data visualization\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# To split the data\\nfrom sklearn.model_selection import train_test_split\\n\\n# To impute missing values\\nfrom sklearn.impute import SimpleImputer\\n\\n# To do one-hot encoding\\nfrom sklearn.preprocessing import OneHotEncoder\\n\\n# To build a decision tree model\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\n# To get different performance metrics\\nimport sklearn.metrics as metrics\\nfrom sklearn.metrics import (\\n    classification_report,\\n    confusion_matrix,\\n    recall_score,\\n    accuracy_score,\\n    precision_score,\\n    f1_score,\\n)\\n\\n\\n# To undersample and oversample the data\\nfrom imblearn.over_sampling import SMOTE\\nfrom imblearn.under_sampling import RandomUnderSampler\\n\\n# To suppress warnings\\nimport warnings\\nwarnings.filterwarnings(\\\"ignore\\\")\";\n                var nbb_formatted_code = \"# has SMOTE\\n# To help with reading and manipulation of data\\nimport numpy as np\\nfrom numpy import array\\nimport pandas as pd\\n\\n# To help with data visualization\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# To split the data\\nfrom sklearn.model_selection import train_test_split\\n\\n# To impute missing values\\nfrom sklearn.impute import SimpleImputer\\n\\n# To do one-hot encoding\\nfrom sklearn.preprocessing import OneHotEncoder\\n\\n# To build a decision tree model\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\n# To get different performance metrics\\nimport sklearn.metrics as metrics\\nfrom sklearn.metrics import (\\n    classification_report,\\n    confusion_matrix,\\n    recall_score,\\n    accuracy_score,\\n    precision_score,\\n    f1_score,\\n)\\n\\n\\n# To undersample and oversample the data\\nfrom imblearn.over_sampling import SMOTE\\nfrom imblearn.under_sampling import RandomUnderSampler\\n\\n# To suppress warnings\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 86;\n                var nbb_unformatted_code = \"# has SMOTE\\n# To help with reading and manipulation of data\\nimport numpy as np\\nfrom numpy import array\\nimport pandas as pd\\n\\n# To help with data visualization\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# To split the data\\nfrom sklearn.model_selection import train_test_split\\n\\n# To impute missing values\\nfrom sklearn.impute import SimpleImputer\\n\\n# To do one-hot encoding\\nfrom sklearn.preprocessing import OneHotEncoder\\n\\n# To build a decision tree model\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\n# To get different performance metrics\\nimport sklearn.metrics as metrics\\nfrom sklearn.metrics import (\\n    classification_report,\\n    confusion_matrix,\\n    recall_score,\\n    accuracy_score,\\n    precision_score,\\n    f1_score,\\n)\\n\\n\\n# To undersample and oversample the data\\nfrom imblearn.over_sampling import SMOTE\\nfrom imblearn.under_sampling import RandomUnderSampler\\n\\n# To suppress warnings\\nimport warnings\\nwarnings.filterwarnings(\\\"ignore\\\")\";\n                var nbb_formatted_code = \"# has SMOTE\\n# To help with reading and manipulation of data\\nimport numpy as np\\nfrom numpy import array\\nimport pandas as pd\\n\\n# To help with data visualization\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# To split the data\\nfrom sklearn.model_selection import train_test_split\\n\\n# To impute missing values\\nfrom sklearn.impute import SimpleImputer\\n\\n# To do one-hot encoding\\nfrom sklearn.preprocessing import OneHotEncoder\\n\\n# To build a decision tree model\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\n# To get different performance metrics\\nimport sklearn.metrics as metrics\\nfrom sklearn.metrics import (\\n    classification_report,\\n    confusion_matrix,\\n    recall_score,\\n    accuracy_score,\\n    precision_score,\\n    f1_score,\\n)\\n\\n\\n# To undersample and oversample the data\\nfrom imblearn.over_sampling import SMOTE\\nfrom imblearn.under_sampling import RandomUnderSampler\\n\\n# To suppress warnings\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 86;\n                var nbb_unformatted_code = \"# has SMOTE\\n# To help with reading and manipulation of data\\nimport numpy as np\\nfrom numpy import array\\nimport pandas as pd\\n\\n# To help with data visualization\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# To split the data\\nfrom sklearn.model_selection import train_test_split\\n\\n# To impute missing values\\nfrom sklearn.impute import SimpleImputer\\n\\n# To do one-hot encoding\\nfrom sklearn.preprocessing import OneHotEncoder\\n\\n# To build a decision tree model\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\n# To get different performance metrics\\nimport sklearn.metrics as metrics\\nfrom sklearn.metrics import (\\n    classification_report,\\n    confusion_matrix,\\n    recall_score,\\n    accuracy_score,\\n    precision_score,\\n    f1_score,\\n)\\n\\n\\n# To undersample and oversample the data\\nfrom imblearn.over_sampling import SMOTE\\nfrom imblearn.under_sampling import RandomUnderSampler\\n\\n# To suppress warnings\\nimport warnings\\nwarnings.filterwarnings(\\\"ignore\\\")\";\n                var nbb_formatted_code = \"# has SMOTE\\n# To help with reading and manipulation of data\\nimport numpy as np\\nfrom numpy import array\\nimport pandas as pd\\n\\n# To help with data visualization\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# To split the data\\nfrom sklearn.model_selection import train_test_split\\n\\n# To impute missing values\\nfrom sklearn.impute import SimpleImputer\\n\\n# To do one-hot encoding\\nfrom sklearn.preprocessing import OneHotEncoder\\n\\n# To build a decision tree model\\nfrom sklearn.tree import DecisionTreeClassifier\\n\\n# To get different performance metrics\\nimport sklearn.metrics as metrics\\nfrom sklearn.metrics import (\\n    classification_report,\\n    confusion_matrix,\\n    recall_score,\\n    accuracy_score,\\n    precision_score,\\n    f1_score,\\n)\\n\\n\\n# To undersample and oversample the data\\nfrom imblearn.over_sampling import SMOTE\\nfrom imblearn.under_sampling import RandomUnderSampler\\n\\n# To suppress warnings\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# has SMOTE\n",
        "# To help with reading and manipulation of data\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "\n",
        "# To help with data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# To split the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# To impute missing values\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# To do one-hot encoding\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# To build a decision tree model\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# To get different performance metrics\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    recall_score,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    f1_score,\n",
        ")\n",
        "\n",
        "\n",
        "# To undersample and oversample the data\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# To suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "end pre-code inner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 87;\n                var nbb_unformatted_code = \"df = pd.read_csv(\\\"/workspaces/codespaces-jupyter/data/Train.csv\\\")\";\n                var nbb_formatted_code = \"df = pd.read_csv(\\\"/workspaces/codespaces-jupyter/data/Train.csv\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 87;\n                var nbb_unformatted_code = \"df = pd.read_csv(\\\"/workspaces/codespaces-jupyter/data/Train.csv\\\")\";\n                var nbb_formatted_code = \"df = pd.read_csv(\\\"/workspaces/codespaces-jupyter/data/Train.csv\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 87;\n                var nbb_unformatted_code = \"df = pd.read_csv(\\\"/workspaces/codespaces-jupyter/data/Train.csv\\\")\";\n                var nbb_formatted_code = \"df = pd.read_csv(\\\"/workspaces/codespaces-jupyter/data/Train.csv\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 87;\n                var nbb_unformatted_code = \"df = pd.read_csv(\\\"/workspaces/codespaces-jupyter/data/Train.csv\\\")\";\n                var nbb_formatted_code = \"df = pd.read_csv(\\\"/workspaces/codespaces-jupyter/data/Train.csv\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df = pd.read_csv(\"/workspaces/codespaces-jupyter/data/Train.csv\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "end pre code outer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "dbiVVX6hbp0v"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 90;\n                var nbb_unformatted_code = \"# Libraries to help with reading and manipulating data\\nimport pandas as pd\\nimport numpy as np\\n\\n# Libaries to help with data visualization\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# To tune model, get different metric scores, and split data\\nfrom sklearn.metrics import (\\n    f1_score,\\n    accuracy_score,\\n    recall_score,\\n    precision_score,\\n    confusion_matrix,\\n    roc_auc_score,\\n    ConfusionMatrixDisplay,\\n)\\nfrom sklearn import metrics\\n\\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\\n\\n# To be used for data scaling and one hot encoding\\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\\n\\n# To impute missing values\\nfrom sklearn.impute import SimpleImputer\\n\\n# To oversample and undersample data\\nfrom imblearn.over_sampling import SMOTE\\nfrom imblearn.under_sampling import RandomUnderSampler\\n\\n# To do hyperparameter tuning\\nfrom sklearn.model_selection import RandomizedSearchCV\\n\\n# To be used for creating pipelines and personalizing them\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.compose import ColumnTransformer\\n\\n# To define maximum number of columns to be displayed in a dataframe\\npd.set_option(\\\"display.max_columns\\\", None)\\npd.set_option(\\\"display.max_rows\\\", None)\\n\\n# To supress scientific notations for a dataframe\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.3f\\\" % x)\\n\\n# To help with model building\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.ensemble import (\\n    AdaBoostClassifier,\\n    GradientBoostingClassifier,\\n    RandomForestClassifier,\\n    BaggingClassifier,\\n)\\nfrom xgboost import XGBClassifier\\n\\n# To suppress scientific notations\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.3f\\\" % x)\\n\\n# To suppress warnings\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# This will help in making the Python code more structured automatically (good coding practice)\\n#%load_ext nb_black\";\n                var nbb_formatted_code = \"# Libraries to help with reading and manipulating data\\nimport pandas as pd\\nimport numpy as np\\n\\n# Libaries to help with data visualization\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# To tune model, get different metric scores, and split data\\nfrom sklearn.metrics import (\\n    f1_score,\\n    accuracy_score,\\n    recall_score,\\n    precision_score,\\n    confusion_matrix,\\n    roc_auc_score,\\n    ConfusionMatrixDisplay,\\n)\\nfrom sklearn import metrics\\n\\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\\n\\n# To be used for data scaling and one hot encoding\\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\\n\\n# To impute missing values\\nfrom sklearn.impute import SimpleImputer\\n\\n# To oversample and undersample data\\nfrom imblearn.over_sampling import SMOTE\\nfrom imblearn.under_sampling import RandomUnderSampler\\n\\n# To do hyperparameter tuning\\nfrom sklearn.model_selection import RandomizedSearchCV\\n\\n# To be used for creating pipelines and personalizing them\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.compose import ColumnTransformer\\n\\n# To define maximum number of columns to be displayed in a dataframe\\npd.set_option(\\\"display.max_columns\\\", None)\\npd.set_option(\\\"display.max_rows\\\", None)\\n\\n# To supress scientific notations for a dataframe\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.3f\\\" % x)\\n\\n# To help with model building\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.ensemble import (\\n    AdaBoostClassifier,\\n    GradientBoostingClassifier,\\n    RandomForestClassifier,\\n    BaggingClassifier,\\n)\\nfrom xgboost import XGBClassifier\\n\\n# To suppress scientific notations\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.3f\\\" % x)\\n\\n# To suppress warnings\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# This will help in making the Python code more structured automatically (good coding practice)\\n# %load_ext nb_black\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 90;\n                var nbb_unformatted_code = \"# Libraries to help with reading and manipulating data\\nimport pandas as pd\\nimport numpy as np\\n\\n# Libaries to help with data visualization\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# To tune model, get different metric scores, and split data\\nfrom sklearn.metrics import (\\n    f1_score,\\n    accuracy_score,\\n    recall_score,\\n    precision_score,\\n    confusion_matrix,\\n    roc_auc_score,\\n    ConfusionMatrixDisplay,\\n)\\nfrom sklearn import metrics\\n\\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\\n\\n# To be used for data scaling and one hot encoding\\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\\n\\n# To impute missing values\\nfrom sklearn.impute import SimpleImputer\\n\\n# To oversample and undersample data\\nfrom imblearn.over_sampling import SMOTE\\nfrom imblearn.under_sampling import RandomUnderSampler\\n\\n# To do hyperparameter tuning\\nfrom sklearn.model_selection import RandomizedSearchCV\\n\\n# To be used for creating pipelines and personalizing them\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.compose import ColumnTransformer\\n\\n# To define maximum number of columns to be displayed in a dataframe\\npd.set_option(\\\"display.max_columns\\\", None)\\npd.set_option(\\\"display.max_rows\\\", None)\\n\\n# To supress scientific notations for a dataframe\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.3f\\\" % x)\\n\\n# To help with model building\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.ensemble import (\\n    AdaBoostClassifier,\\n    GradientBoostingClassifier,\\n    RandomForestClassifier,\\n    BaggingClassifier,\\n)\\nfrom xgboost import XGBClassifier\\n\\n# To suppress scientific notations\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.3f\\\" % x)\\n\\n# To suppress warnings\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# This will help in making the Python code more structured automatically (good coding practice)\\n#%load_ext nb_black\";\n                var nbb_formatted_code = \"# Libraries to help with reading and manipulating data\\nimport pandas as pd\\nimport numpy as np\\n\\n# Libaries to help with data visualization\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# To tune model, get different metric scores, and split data\\nfrom sklearn.metrics import (\\n    f1_score,\\n    accuracy_score,\\n    recall_score,\\n    precision_score,\\n    confusion_matrix,\\n    roc_auc_score,\\n    ConfusionMatrixDisplay,\\n)\\nfrom sklearn import metrics\\n\\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\\n\\n# To be used for data scaling and one hot encoding\\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\\n\\n# To impute missing values\\nfrom sklearn.impute import SimpleImputer\\n\\n# To oversample and undersample data\\nfrom imblearn.over_sampling import SMOTE\\nfrom imblearn.under_sampling import RandomUnderSampler\\n\\n# To do hyperparameter tuning\\nfrom sklearn.model_selection import RandomizedSearchCV\\n\\n# To be used for creating pipelines and personalizing them\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.compose import ColumnTransformer\\n\\n# To define maximum number of columns to be displayed in a dataframe\\npd.set_option(\\\"display.max_columns\\\", None)\\npd.set_option(\\\"display.max_rows\\\", None)\\n\\n# To supress scientific notations for a dataframe\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.3f\\\" % x)\\n\\n# To help with model building\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.ensemble import (\\n    AdaBoostClassifier,\\n    GradientBoostingClassifier,\\n    RandomForestClassifier,\\n    BaggingClassifier,\\n)\\nfrom xgboost import XGBClassifier\\n\\n# To suppress scientific notations\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.3f\\\" % x)\\n\\n# To suppress warnings\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# This will help in making the Python code more structured automatically (good coding practice)\\n# %load_ext nb_black\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 90;\n                var nbb_unformatted_code = \"# Libraries to help with reading and manipulating data\\nimport pandas as pd\\nimport numpy as np\\n\\n# Libaries to help with data visualization\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# To tune model, get different metric scores, and split data\\nfrom sklearn.metrics import (\\n    f1_score,\\n    accuracy_score,\\n    recall_score,\\n    precision_score,\\n    confusion_matrix,\\n    roc_auc_score,\\n    ConfusionMatrixDisplay,\\n)\\nfrom sklearn import metrics\\n\\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\\n\\n# To be used for data scaling and one hot encoding\\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\\n\\n# To impute missing values\\nfrom sklearn.impute import SimpleImputer\\n\\n# To oversample and undersample data\\nfrom imblearn.over_sampling import SMOTE\\nfrom imblearn.under_sampling import RandomUnderSampler\\n\\n# To do hyperparameter tuning\\nfrom sklearn.model_selection import RandomizedSearchCV\\n\\n# To be used for creating pipelines and personalizing them\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.compose import ColumnTransformer\\n\\n# To define maximum number of columns to be displayed in a dataframe\\npd.set_option(\\\"display.max_columns\\\", None)\\npd.set_option(\\\"display.max_rows\\\", None)\\n\\n# To supress scientific notations for a dataframe\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.3f\\\" % x)\\n\\n# To help with model building\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.ensemble import (\\n    AdaBoostClassifier,\\n    GradientBoostingClassifier,\\n    RandomForestClassifier,\\n    BaggingClassifier,\\n)\\nfrom xgboost import XGBClassifier\\n\\n# To suppress scientific notations\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.3f\\\" % x)\\n\\n# To suppress warnings\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# This will help in making the Python code more structured automatically (good coding practice)\\n#%load_ext nb_black\";\n                var nbb_formatted_code = \"# Libraries to help with reading and manipulating data\\nimport pandas as pd\\nimport numpy as np\\n\\n# Libaries to help with data visualization\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# To tune model, get different metric scores, and split data\\nfrom sklearn.metrics import (\\n    f1_score,\\n    accuracy_score,\\n    recall_score,\\n    precision_score,\\n    confusion_matrix,\\n    roc_auc_score,\\n    ConfusionMatrixDisplay,\\n)\\nfrom sklearn import metrics\\n\\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\\n\\n# To be used for data scaling and one hot encoding\\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\\n\\n# To impute missing values\\nfrom sklearn.impute import SimpleImputer\\n\\n# To oversample and undersample data\\nfrom imblearn.over_sampling import SMOTE\\nfrom imblearn.under_sampling import RandomUnderSampler\\n\\n# To do hyperparameter tuning\\nfrom sklearn.model_selection import RandomizedSearchCV\\n\\n# To be used for creating pipelines and personalizing them\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.compose import ColumnTransformer\\n\\n# To define maximum number of columns to be displayed in a dataframe\\npd.set_option(\\\"display.max_columns\\\", None)\\npd.set_option(\\\"display.max_rows\\\", None)\\n\\n# To supress scientific notations for a dataframe\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.3f\\\" % x)\\n\\n# To help with model building\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.ensemble import (\\n    AdaBoostClassifier,\\n    GradientBoostingClassifier,\\n    RandomForestClassifier,\\n    BaggingClassifier,\\n)\\nfrom xgboost import XGBClassifier\\n\\n# To suppress scientific notations\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.3f\\\" % x)\\n\\n# To suppress warnings\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# This will help in making the Python code more structured automatically (good coding practice)\\n# %load_ext nb_black\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 90;\n                var nbb_unformatted_code = \"# Libraries to help with reading and manipulating data\\nimport pandas as pd\\nimport numpy as np\\n\\n# Libaries to help with data visualization\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# To tune model, get different metric scores, and split data\\nfrom sklearn.metrics import (\\n    f1_score,\\n    accuracy_score,\\n    recall_score,\\n    precision_score,\\n    confusion_matrix,\\n    roc_auc_score,\\n    ConfusionMatrixDisplay,\\n)\\nfrom sklearn import metrics\\n\\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\\n\\n# To be used for data scaling and one hot encoding\\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\\n\\n# To impute missing values\\nfrom sklearn.impute import SimpleImputer\\n\\n# To oversample and undersample data\\nfrom imblearn.over_sampling import SMOTE\\nfrom imblearn.under_sampling import RandomUnderSampler\\n\\n# To do hyperparameter tuning\\nfrom sklearn.model_selection import RandomizedSearchCV\\n\\n# To be used for creating pipelines and personalizing them\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.compose import ColumnTransformer\\n\\n# To define maximum number of columns to be displayed in a dataframe\\npd.set_option(\\\"display.max_columns\\\", None)\\npd.set_option(\\\"display.max_rows\\\", None)\\n\\n# To supress scientific notations for a dataframe\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.3f\\\" % x)\\n\\n# To help with model building\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.ensemble import (\\n    AdaBoostClassifier,\\n    GradientBoostingClassifier,\\n    RandomForestClassifier,\\n    BaggingClassifier,\\n)\\nfrom xgboost import XGBClassifier\\n\\n# To suppress scientific notations\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.3f\\\" % x)\\n\\n# To suppress warnings\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# This will help in making the Python code more structured automatically (good coding practice)\\n#%load_ext nb_black\";\n                var nbb_formatted_code = \"# Libraries to help with reading and manipulating data\\nimport pandas as pd\\nimport numpy as np\\n\\n# Libaries to help with data visualization\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# To tune model, get different metric scores, and split data\\nfrom sklearn.metrics import (\\n    f1_score,\\n    accuracy_score,\\n    recall_score,\\n    precision_score,\\n    confusion_matrix,\\n    roc_auc_score,\\n    ConfusionMatrixDisplay,\\n)\\nfrom sklearn import metrics\\n\\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\\n\\n# To be used for data scaling and one hot encoding\\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\\n\\n# To impute missing values\\nfrom sklearn.impute import SimpleImputer\\n\\n# To oversample and undersample data\\nfrom imblearn.over_sampling import SMOTE\\nfrom imblearn.under_sampling import RandomUnderSampler\\n\\n# To do hyperparameter tuning\\nfrom sklearn.model_selection import RandomizedSearchCV\\n\\n# To be used for creating pipelines and personalizing them\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.compose import ColumnTransformer\\n\\n# To define maximum number of columns to be displayed in a dataframe\\npd.set_option(\\\"display.max_columns\\\", None)\\npd.set_option(\\\"display.max_rows\\\", None)\\n\\n# To supress scientific notations for a dataframe\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.3f\\\" % x)\\n\\n# To help with model building\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.ensemble import (\\n    AdaBoostClassifier,\\n    GradientBoostingClassifier,\\n    RandomForestClassifier,\\n    BaggingClassifier,\\n)\\nfrom xgboost import XGBClassifier\\n\\n# To suppress scientific notations\\npd.set_option(\\\"display.float_format\\\", lambda x: \\\"%.3f\\\" % x)\\n\\n# To suppress warnings\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# This will help in making the Python code more structured automatically (good coding practice)\\n# %load_ext nb_black\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Libraries to help with reading and manipulating data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Libaries to help with data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# To tune model, get different metric scores, and split data\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    accuracy_score,\n",
        "    recall_score,\n",
        "    precision_score,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    ConfusionMatrixDisplay,\n",
        ")\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "\n",
        "# To be used for data scaling and one hot encoding\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
        "\n",
        "# To impute missing values\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# To oversample and undersample data\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# To do hyperparameter tuning\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# To be used for creating pipelines and personalizing them\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# To define maximum number of columns to be displayed in a dataframe\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.max_rows\", None)\n",
        "\n",
        "# To supress scientific notations for a dataframe\n",
        "pd.set_option(\"display.float_format\", lambda x: \"%.3f\" % x)\n",
        "\n",
        "# To help with model building\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import (\n",
        "    AdaBoostClassifier,\n",
        "    GradientBoostingClassifier,\n",
        "    RandomForestClassifier,\n",
        "    BaggingClassifier,\n",
        ")\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# To suppress scientific notations\n",
        "pd.set_option(\"display.float_format\", lambda x: \"%.3f\" % x)\n",
        "\n",
        "# To suppress warnings\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# This will help in making the Python code more structured automatically (good coding practice)\n",
        "#%load_ext nb_black"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxhpZv9y-qTw"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39UgpBY3bp0y"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('_______') ##  Complete the code to read the data\n",
        "df_test = pd.read_csv('_______') ##  Complete the code to read the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1TEog7cAs5m"
      },
      "source": [
        "## Data Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-5jUOgu-qTz"
      },
      "source": [
        "The initial steps to get an overview of any dataset is to: \n",
        "- observe the first few rows of the dataset, to check whether the dataset has been loaded properly or not\n",
        "- get information about the number of rows and columns in the dataset\n",
        "- find out the data types of the columns to ensure that data is stored in the preferred format and the value of each property is as expected.\n",
        "- check the statistical summary of the dataset to get an overview of the numerical columns of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQi5ygTC-qT1"
      },
      "source": [
        "### Checking the shape of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Y8epHpjbp0z"
      },
      "outputs": [],
      "source": [
        "# Checking the number of rows and columns in the training data\n",
        "df.'_______' ##  Complete the code to view dimensions of the train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hA15qTjzbp01"
      },
      "outputs": [],
      "source": [
        "# Checking the number of rows and columns in the test data\n",
        "df_test.'_______' ##  Complete the code to view dimensions of the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTsYJZBubp02"
      },
      "outputs": [],
      "source": [
        "# let's create a copy of the training data\n",
        "data = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69ImWbLXbp03"
      },
      "outputs": [],
      "source": [
        "# let's create a copy of the training data\n",
        "data_test = df_test.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "961px703qhLV"
      },
      "source": [
        "### Displaying the first few rows of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9RnN7Twbp03"
      },
      "outputs": [],
      "source": [
        "# let's view the first 5 rows of the data\n",
        "data.'_______' ##  Complete the code to view top 5 rows of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9Ir8YPgbp04"
      },
      "outputs": [],
      "source": [
        "# let's view the last 5 rows of the data\n",
        "data_test.'_______' ##  Complete the code to view last 5 rows of the data  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TcqcxbK-qT3"
      },
      "source": [
        "### Checking the data types of the columns for the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXBYJoKkbp04"
      },
      "outputs": [],
      "source": [
        "# let's check the data types of the columns in the dataset\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNr4bWoM-qT5"
      },
      "source": [
        "### Checking for duplicate values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0EmBHNmbp04"
      },
      "outputs": [],
      "source": [
        "# let's check for duplicate values in the data\n",
        "data.'_______' ##  Complete the code to check duplicate entries in the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch_TjRfF-qT5"
      },
      "source": [
        "### Checking for missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlwFZm-Jbp05"
      },
      "outputs": [],
      "source": [
        "# let's check for missing values in the data\n",
        "data.'_______' ##  Complete the code to check missing entries in the train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33bar6robp06"
      },
      "outputs": [],
      "source": [
        "# let's check for missing values in the data\n",
        "data_test.'_______' ##  Complete the code to check missing entries in the test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUCorhch-qT4"
      },
      "source": [
        "### Statistical summary of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6lzvHKCbp06"
      },
      "outputs": [],
      "source": [
        "# let's view the statistical summary of the numerical columns in the data\n",
        "data.'_______' ##  Complete the code to print the statitical summary of the train data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhPuzWO7hmV8"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv7Bs8aUbp07"
      },
      "source": [
        "### Univariate analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIP4bI3Zbp07",
        "outputId": "f5372b09-92ef-4fbc-9398-e267e9116aa9"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 15;\n                var nbb_unformatted_code = \"# function to plot a boxplot and a histogram along the same scale.\\n\\n\\ndef histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):\\n    \\\"\\\"\\\"\\n    Boxplot and histogram combined\\n\\n    data: dataframe\\n    feature: dataframe column\\n    figsize: size of figure (default (12,7))\\n    kde: whether to the show density curve (default False)\\n    bins: number of bins for histogram (default None)\\n    \\\"\\\"\\\"\\n    f2, (ax_box2, ax_hist2) = plt.subplots(\\n        nrows=2,  # Number of rows of the subplot grid= 2\\n        sharex=True,  # x-axis will be shared among all subplots\\n        gridspec_kw={\\\"height_ratios\\\": (0.25, 0.75)},\\n        figsize=figsize,\\n    )  # creating the 2 subplots\\n    sns.boxplot(\\n        data=data, x=feature, ax=ax_box2, showmeans=True, color=\\\"violet\\\"\\n    )  # boxplot will be created and a star will indicate the mean value of the column\\n    sns.histplot(\\n        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette=\\\"winter\\\"\\n    ) if bins else sns.histplot(\\n        data=data, x=feature, kde=kde, ax=ax_hist2\\n    )  # For histogram\\n    ax_hist2.axvline(\\n        data[feature].mean(), color=\\\"green\\\", linestyle=\\\"--\\\"\\n    )  # Add mean to the histogram\\n    ax_hist2.axvline(\\n        data[feature].median(), color=\\\"black\\\", linestyle=\\\"-\\\"\\n    )  # Add median to the histogram\";\n                var nbb_formatted_code = \"# function to plot a boxplot and a histogram along the same scale.\\n\\n\\ndef histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):\\n    \\\"\\\"\\\"\\n    Boxplot and histogram combined\\n\\n    data: dataframe\\n    feature: dataframe column\\n    figsize: size of figure (default (12,7))\\n    kde: whether to the show density curve (default False)\\n    bins: number of bins for histogram (default None)\\n    \\\"\\\"\\\"\\n    f2, (ax_box2, ax_hist2) = plt.subplots(\\n        nrows=2,  # Number of rows of the subplot grid= 2\\n        sharex=True,  # x-axis will be shared among all subplots\\n        gridspec_kw={\\\"height_ratios\\\": (0.25, 0.75)},\\n        figsize=figsize,\\n    )  # creating the 2 subplots\\n    sns.boxplot(\\n        data=data, x=feature, ax=ax_box2, showmeans=True, color=\\\"violet\\\"\\n    )  # boxplot will be created and a star will indicate the mean value of the column\\n    sns.histplot(\\n        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette=\\\"winter\\\"\\n    ) if bins else sns.histplot(\\n        data=data, x=feature, kde=kde, ax=ax_hist2\\n    )  # For histogram\\n    ax_hist2.axvline(\\n        data[feature].mean(), color=\\\"green\\\", linestyle=\\\"--\\\"\\n    )  # Add mean to the histogram\\n    ax_hist2.axvline(\\n        data[feature].median(), color=\\\"black\\\", linestyle=\\\"-\\\"\\n    )  # Add median to the histogram\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# function to plot a boxplot and a histogram along the same scale.\n",
        "\n",
        "\n",
        "def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):\n",
        "    \"\"\"\n",
        "    Boxplot and histogram combined\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    figsize: size of figure (default (12,7))\n",
        "    kde: whether to the show density curve (default False)\n",
        "    bins: number of bins for histogram (default None)\n",
        "    \"\"\"\n",
        "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
        "        nrows=2,  # Number of rows of the subplot grid= 2\n",
        "        sharex=True,  # x-axis will be shared among all subplots\n",
        "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
        "        figsize=figsize,\n",
        "    )  # creating the 2 subplots\n",
        "    sns.boxplot(\n",
        "        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
        "    )  # boxplot will be created and a triangle will indicate the mean value of the column\n",
        "    sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette=\"winter\"\n",
        "    ) if bins else sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2\n",
        "    )  # For histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].mean(), color=\"green\", linestyle=\"--\"\n",
        "    )  # Add mean to the histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].median(), color=\"black\", linestyle=\"-\"\n",
        "    )  # Add median to the histogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBkL6zw0bp07"
      },
      "source": [
        "#### Plotting histograms and boxplots for all the variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0ip-InCbp07",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "for feature in df.columns:\n",
        "    histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfvnghrACelD"
      },
      "source": [
        "### Let's look at the values in target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jchAb6Ikbp08"
      },
      "outputs": [],
      "source": [
        "data[\"Target\"].'_______' ##  Complete the code to check the class distribution in target variable for train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rupsM4sbp08"
      },
      "outputs": [],
      "source": [
        "data_test[\"Target\"].'_______' ##  Complete the code to check the class distribution in target variable for test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp8vC9MZbp09"
      },
      "source": [
        "## Data Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IktO9G2Lbp09"
      },
      "outputs": [],
      "source": [
        "# Dividing train data into X and y \n",
        "X = data.drop([\"Target\"], axis=1)\n",
        "y = data[\"Target\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBXiW4s4kd63"
      },
      "source": [
        "**Since we already have a separate test set, we don't need to divide data into train, valiation and test**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-8vqBMXbp09"
      },
      "outputs": [],
      "source": [
        "# Splitting train dataset into training and validation set\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split('_______') ## Complete the code to split the train dataset into train test in the ratio 75:25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDmoKnBBFEpG"
      },
      "outputs": [],
      "source": [
        "# Checking the number of rows and columns in the X_train data\n",
        "X_train.'_______' ##  Complete the code to view dimensions of the X_train data\n",
        "\n",
        "# Checking the number of rows and columns in the X_val data\n",
        "X_val.'_______' ##  Complete the code to view dimensions of the X_val data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okpvXPeRkTL2"
      },
      "outputs": [],
      "source": [
        "# Dividing test data into X_test and y_test\n",
        "\n",
        "X_test = data_test.'_______' ##  Complete the code to drop target variable from test data\n",
        "y_test = data_test'_______' ##  Complete the code to store target variable in y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lk7gMVMSjR0N"
      },
      "outputs": [],
      "source": [
        "# Checking the number of rows and columns in the X_test data\n",
        "X_test.'_______' ##  Complete the code to view dimensions of the X_test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J99-7Kubp09"
      },
      "source": [
        "## Missing value imputation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28L1vgAwbp09"
      },
      "outputs": [],
      "source": [
        "# creating an instace of the imputer to be used\n",
        "imputer = SimpleImputer(strategy=\"median\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNfbw4rJbp09"
      },
      "outputs": [],
      "source": [
        "# Fit and transform the train data\n",
        "X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
        "\n",
        "# Transform the validation data\n",
        "X_val = pd.DataFrame(imputer._______(X_val), columns=X_train.columns) ## Complete the code to impute missing values in X_val without data leakage\n",
        "\n",
        "# Transform the test data\n",
        "X_test = pd.DataFrame(imputer._______(X_test), columns=X_train.columns) ## Complete the code to impute missing values in X_test without data leakage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jdLvwTAbp09"
      },
      "outputs": [],
      "source": [
        "# Checking that no column has missing values in train or test sets\n",
        "print(X_train.isna().sum())\n",
        "print(\"-\" * 30)\n",
        "\n",
        "X_val.'_______' ## Complete the code to check the count of missing values in validation set\n",
        "X_test.'_______' ## Complete the code to check the count of missing values in test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzOa9FGA6WtG"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZqmoqz7bp0-"
      },
      "source": [
        "### Model evaluation criterion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2ORUgmUjDZC"
      },
      "source": [
        "The nature of predictions made by the classification model will translate as follows:\n",
        "\n",
        "- True positives (TP) are failures correctly predicted by the model.\n",
        "- False negatives (FN) are real failures in a generator where there is no detection by model. \n",
        "- False positives (FP) are failure detections in a generator where there is no failure.\n",
        "\n",
        "**Which metric to optimize?**\n",
        "\n",
        "* We need to choose the metric which will ensure that the maximum number of generator failures are predicted correctly by the model.\n",
        "* We would want Recall to be maximized as greater the Recall, the higher the chances of minimizing false negatives.\n",
        "* We want to minimize false negatives because if a model predicts that a machine will have no failure when there will be a failure, it will increase the maintenance cost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUs837UXbp0-"
      },
      "source": [
        "**Let's define a function to output different metrics (including recall) on the train and test set and a function to show confusion matrix so that we do not have to use the same code repetitively while evaluating models.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nt33nwx_bp0-"
      },
      "outputs": [],
      "source": [
        "# defining a function to compute different metrics to check performance of a classification model built using sklearn\n",
        "def model_performance_classification_sklearn(model, predictors, target):\n",
        "    \"\"\"\n",
        "    Function to compute different metrics to check classification model performance\n",
        "\n",
        "    model: classifier\n",
        "    predictors: independent variables\n",
        "    target: dependent variable\n",
        "    \"\"\"\n",
        "\n",
        "    # predicting using the independent variables\n",
        "    pred = model.predict(predictors)\n",
        "\n",
        "    acc = accuracy_score(target, pred)  # to compute Accuracy\n",
        "    recall = recall_score(target, pred)  # to compute Recall\n",
        "    precision = precision_score(target, pred)  # to compute Precision\n",
        "    f1 = f1_score(target, pred)  # to compute F1-score\n",
        "\n",
        "    # creating a dataframe of metrics\n",
        "    df_perf = pd.DataFrame(\n",
        "        {\n",
        "            \"Accuracy\": acc,\n",
        "            \"Recall\": recall,\n",
        "            \"Precision\": precision,\n",
        "            \"F1\": f1\n",
        "        },\n",
        "        index=[0],\n",
        "    )\n",
        "\n",
        "    return df_perf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hOzddAXbp0_"
      },
      "source": [
        "### Defining scorer to be used for cross-validation and hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzLUm-eSi_cJ"
      },
      "source": [
        "- We want to reduce false negatives and will try to maximize \"Recall\".\n",
        "- To maximize Recall, we can use Recall as a **scorer** in cross-validation and hyperparameter tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayLcyOLsbp0_"
      },
      "outputs": [],
      "source": [
        "# Type of scoring used to compare parameter combinations\n",
        "scorer = metrics.make_scorer(metrics.recall_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNwROa3X3OV7"
      },
      "source": [
        "**We are now done with pre-processing and evaluation criterion, so let's start building the model.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Je_G2zN3VCF"
      },
      "source": [
        "### Model Building on original data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OTOG3o1bp0_",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "models = []  # Empty list to store all the models\n",
        "\n",
        "# Appending models into the list\n",
        "models.append((\"Logistic regression\", LogisticRegression(random_state=1)))\n",
        "models.append((\"Bagging\", BaggingClassifier(random_state=1)))\n",
        "'_______' ## Complete the code to append remaining 4 models in the list models\n",
        "\n",
        "results1 = []  # Empty list to store all model's CV scores\n",
        "names = []  # Empty list to store name of the models\n",
        "\n",
        "\n",
        "# loop through all models to get the mean cross validated score\n",
        "print(\"\\n\" \"Cross-Validation performance on training dataset:\" \"\\n\")\n",
        "\n",
        "for name, model in models:\n",
        "    kfold = StratifiedKFold(\n",
        "        n_splits=5, shuffle=True, random_state=1\n",
        "    )  # Setting number of splits equal to 5\n",
        "    cv_result = cross_val_score(\n",
        "        estimator=model, X=X_train, y=y_train, scoring=scorer, cv=kfold\n",
        "    )\n",
        "    results1.append(cv_result)\n",
        "    names.append(name)\n",
        "    print(\"{}: {}\".format(name, cv_result.mean()))\n",
        "\n",
        "print(\"\\n\" \"Validation Performance:\" \"\\n\")\n",
        "\n",
        "for name, model in models:\n",
        "    model.fit(X_train, y_train)\n",
        "    scores = recall_score(y_val, model.predict(X_val))\n",
        "    print(\"{}: {}\".format(name, scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cm61fR_mbp0_",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Plotting boxplots for CV scores of all models defined above\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "\n",
        "fig.suptitle(\"Algorithm Comparison\")\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "plt.boxplot(results1)\n",
        "ax.set_xticklabels(names)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C91_6Swtbp1A"
      },
      "source": [
        "### Model Building with oversampled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2dEk01hbp1A"
      },
      "outputs": [],
      "source": [
        "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1)))\n",
        "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0)))\n",
        "\n",
        "# Synthetic Minority Over Sampling Technique\n",
        "sm = SMOTE(sampling_strategy=1, k_neighbors=5, random_state=1)\n",
        "X_train_over, y_train_over = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "\n",
        "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_over == 1)))\n",
        "print(\"After OverSampling, counts of label '0': {} \\n\".format(sum(y_train_over == 0)))\n",
        "\n",
        "\n",
        "print(\"After OverSampling, the shape of train_X: {}\".format(X_train_over.shape))\n",
        "print(\"After OverSampling, the shape of train_y: {} \\n\".format(y_train_over.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMdOjn_uICGt"
      },
      "outputs": [],
      "source": [
        "models = []  # Empty list to store all the models\n",
        "\n",
        "# Appending models into the list\n",
        "models.append((\"Logistic regression\", LogisticRegression(random_state=1)))\n",
        "models.append((\"Bagging\", BaggingClassifier(random_state=1)))\n",
        "'_______' ## Complete the code to append remaining 4 models in the list models\n",
        "\n",
        "results1 = []  # Empty list to store all model's CV scores\n",
        "names = []  # Empty list to store name of the models\n",
        "\n",
        "\n",
        "# loop through all models to get the mean cross validated score\n",
        "print(\"\\n\" \"Cross-Validation performance on training dataset:\" \"\\n\")\n",
        "\n",
        "for name, model in models:\n",
        "    kfold = StratifiedKFold(\n",
        "        n_splits=5, shuffle=True, random_state=1\n",
        "    )  # Setting number of splits equal to 5\n",
        "    cv_result = cross_val_score(\n",
        "        estimator=model, X=_______, y=_______, scoring=scorer, cv=kfold\n",
        "    )  ## Complete the code to build models on oversampled data\n",
        "    results1.append(cv_result)\n",
        "    names.append(name)\n",
        "    print(\"{}: {}\".format(name, cv_result.mean()))\n",
        "\n",
        "print(\"\\n\" \"Validation Performance:\" \"\\n\")\n",
        "\n",
        "for name, model in models:\n",
        "    model.fit(______,______)## Complete the code to build models on oversampled data \n",
        "    scores = recall_score(y_val, model.predict(X_val))\n",
        "    print(\"{}: {}\".format(name, scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwUcBORjbp1A",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Plotting boxplots for CV scores of all models defined above\n",
        "'_______' ## Write the code to create boxplot to check model performance on oversampled data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYLfDmHvbp1B"
      },
      "source": [
        "### Model Building with undersampled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHM-5g8Qbp1B"
      },
      "outputs": [],
      "source": [
        "rus = RandomUnderSampler(random_state=1, sampling_strategy=1)\n",
        "X_train_un, y_train_un = rus.fit_resample(X_train, y_train)\n",
        "\n",
        "\n",
        "print(\"Before UnderSampling, counts of label '1': {}\".format(sum(y_train == 1)))\n",
        "print(\"Before UnderSampling, counts of label '0': {} \\n\".format(sum(y_train == 0)))\n",
        "\n",
        "\n",
        "print(\"After UnderSampling, counts of label '1': {}\".format(sum(y_train_un == 1)))\n",
        "print(\"After UnderSampling, counts of label '0': {} \\n\".format(sum(y_train_un == 0)))\n",
        "\n",
        "\n",
        "print(\"After UnderSampling, the shape of train_X: {}\".format(X_train_un.shape))\n",
        "print(\"After UnderSampling, the shape of train_y: {} \\n\".format(y_train_un.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTcxf6VwIp4o"
      },
      "outputs": [],
      "source": [
        "models = []  # Empty list to store all the models\n",
        "\n",
        "# Appending models into the list\n",
        "models.append((\"Logistic regression\", LogisticRegression(random_state=1)))\n",
        "models.append((\"Bagging\", BaggingClassifier(random_state=1)))\n",
        "'_______' ## Complete the code to append remaining 4 models in the list models\n",
        "\n",
        "results1 = []  # Empty list to store all model's CV scores\n",
        "names = []  # Empty list to store name of the models\n",
        "\n",
        "\n",
        "# loop through all models to get the mean cross validated score\n",
        "print(\"\\n\" \"Cross-Validation performance on training dataset:\" \"\\n\")\n",
        "\n",
        "for name, model in models:\n",
        "    kfold = StratifiedKFold(\n",
        "        n_splits=5, shuffle=True, random_state=1\n",
        "    )  # Setting number of splits equal to 5\n",
        "    cv_result = cross_val_score(\n",
        "        estimator=model, X=_______, y=_______, scoring=scorer, cv=kfold\n",
        "    )  ## Complete the code to build models on undersampled data\n",
        "    results1.append(cv_result)\n",
        "    names.append(name)\n",
        "    print(\"{}: {}\".format(name, cv_result.mean()))\n",
        "\n",
        "print(\"\\n\" \"Validation Performance:\" \"\\n\")\n",
        "\n",
        "for name, model in models:\n",
        "    model.fit(______,______)## Complete the code to build models on undersampled data \n",
        "    scores = recall_score(y_val, model.predict(X_val))\n",
        "    print(\"{}: {}\".format(name, scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HcDqaD2bp1B",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Plotting boxplots for CV scores of all models defined above\n",
        "'_______' ## Write the code to create boxplot to check model performance on undersampled data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERm8IqLo34q8"
      },
      "source": [
        "**After looking at performance of all the models, let's decide which models can further improve with hyperparameter tuning.**\n",
        "\n",
        "**Note**: You can choose to tune some other model if XGBoost gives error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLZlKa99bp1C"
      },
      "source": [
        "## Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFEScXRsl9NM"
      },
      "source": [
        "### **Note**\n",
        "1. Sample parameter grid has been provided to do necessary hyperparameter tuning. One can extend/reduce the parameter grid based on execution time and system configuration to try to improve the model performance further wherever needed.      \n",
        "2. The models chosen in this notebook are based on test runs. One can update the best models as obtained upon code execution and tune them for best performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FtmPS7Ubp1D"
      },
      "source": [
        "### Tuning AdaBoost using oversampled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPSlOlvkbp1D"
      },
      "outputs": [],
      "source": [
        "%%time \n",
        "\n",
        "# defining model\n",
        "Model = AdaBoostClassifier(random_state=1)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid = {\n",
        "    \"n_estimators\": [100, 150, 200],\n",
        "    \"learning_rate\": [0.2, 0.05],\n",
        "    \"base_estimator\": [DecisionTreeClassifier(max_depth=1, random_state=1), DecisionTreeClassifier(max_depth=2, random_state=1), DecisionTreeClassifier(max_depth=3, random_state=1),\n",
        "    ]\n",
        "}\n",
        "\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=50, n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.fit(X_train_over,y_train_over) ## Complete the code to fit the model on over sampled data\n",
        "\n",
        "print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTQNwsr5bp1D"
      },
      "outputs": [],
      "source": [
        "# Creating new pipeline with best parameters\n",
        "tuned_ada = AdaBoostClassifier(\n",
        "    n_estimators= _______, learning_rate= _______, base_estimator= DecisionTreeClassifier(max_depth=_______, random_state=1)\n",
        ") ## Complete the code with the best parameters obtained from tuning\n",
        "\n",
        "tuned_ada.'_______' ## Complete the code to fit the model on oversampled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKzdHSSbbp1D",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "ada_train_perf = model_performance_classification_sklearn(tuned_ada, X_train_over, y_train_over)\n",
        "ada_train_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10_dV5m4bp1D"
      },
      "outputs": [],
      "source": [
        "ada_val_perf = '_______' ## Complete the code to check the performance on validation set\n",
        "ada_val_perf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqj6dc38bp1E"
      },
      "source": [
        "### Tuning Random forest using undersampled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tLNoDYhbp1E"
      },
      "outputs": [],
      "source": [
        "%%time \n",
        "\n",
        "# defining model\n",
        "Model = RandomForestClassifier(random_state=1)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid = {\n",
        "    \"n_estimators\": [200,250,300],\n",
        "    \"min_samples_leaf\": np.arange(1, 4),\n",
        "    \"max_features\": [np.arange(0.3, 0.6, 0.1),'sqrt'],\n",
        "    \"max_samples\": np.arange(0.4, 0.7, 0.1)}\n",
        "\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=50, n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.'_______' ## Complete the code to fit the model on under sampled data\n",
        "\n",
        "print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kmoh9KMbp1E"
      },
      "outputs": [],
      "source": [
        "# Creating new pipeline with best parameters\n",
        "tuned_rf2 = RandomForestClassifier(\n",
        "    max_features=_______,\n",
        "    random_state=1,\n",
        "    max_samples=_______,\n",
        "    n_estimators=_______,\n",
        "    min_samples_leaf=_______,\n",
        ")## Complete the code with the best parameters obtained from tuning\n",
        "\n",
        "tuned_rf2.'_______' ## Complete the code to fit the model on under sampled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAAD5mzybp1E"
      },
      "outputs": [],
      "source": [
        "rf2_train_perf = '_______' ## Complete the code to check the performance on undersampled train set\n",
        "rf2_train_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTtYpzO-bp1E",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "rf2_val_perf = '_______' ## Complete the code to check the performance on validation set\n",
        "rf2_val_perf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoLT8ewJ5V2d"
      },
      "source": [
        "### Tuning Gradient Boosting using oversampled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkDY8_X25Kxe"
      },
      "outputs": [],
      "source": [
        "%%time \n",
        "\n",
        "# defining model\n",
        "Model = GradientBoostingClassifier(random_state=1)\n",
        "\n",
        "#Parameter grid to pass in RandomSearchCV\n",
        "param_grid={\"n_estimators\": np.arange(100,150,25), \"learning_rate\": [0.2, 0.05, 1], \"subsample\":[0.5,0.7], \"max_features\":[0.5,0.7]}\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, scoring=scorer, n_iter=50, n_jobs = -1, cv=5, random_state=1)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.fit(X_train_over, y_train_over)\n",
        "\n",
        "print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_61Omhq5KkB"
      },
      "outputs": [],
      "source": [
        "# Creating new pipeline with best parameters\n",
        "tuned_gbm = GradientBoostingClassifier(\n",
        "    max_features=_______,\n",
        "    random_state=1,\n",
        "    learning_rate=_______,\n",
        "    n_estimators=_______,\n",
        "    subsample=_______,\n",
        ")## Complete the code with the best parameters obtained from tuning\n",
        "\n",
        "tuned_gbm.fit(X_train_over, y_train_over)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8Oxv7PIJTmF"
      },
      "outputs": [],
      "source": [
        "gbm_train_perf = '_______' ## Complete the code to check the performance on oversampled train set\n",
        "gbm_train_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpcbbrBLJjS5"
      },
      "outputs": [],
      "source": [
        "gbm_val_perf = '_______' ## Complete the code to check the performance on validation set\n",
        "gbm_val_perf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbrQHwrKbp1C"
      },
      "source": [
        "### Tuning XGBoost using oversampled data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp_kVMHxiahW"
      },
      "source": [
        "**Note**: You can choose to skip this section if XGBoost gives error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec8e6iVzbp1C"
      },
      "outputs": [],
      "source": [
        "%%time \n",
        "\n",
        "# defining model\n",
        "Model = XGBClassifier(random_state=1,eval_metric='logloss')\n",
        "\n",
        "#Parameter grid to pass in RandomSearchCV\n",
        "param_grid={'n_estimators':[150,200,250],'scale_pos_weight':[5,10], 'learning_rate':[0.1,0.2], 'gamma':[0,3,5], 'subsample':[0.8,0.9]}\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=50, n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.'_______' ## Complete the code to fit the model on over sampled data\n",
        "\n",
        "print(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSio3khJbp1C",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "xgb2 = XGBClassifier(\n",
        "    random_state=1,\n",
        "    eval_metric=\"logloss\",\n",
        "    subsample=_______,\n",
        "    scale_pos_weight=_______,\n",
        "    n_estimators=_______,\n",
        "    learning_rate=_______,\n",
        "    gamma=_______,\n",
        ")## Complete the code with the best parameters obtained from tuning\n",
        "\n",
        "xgb2.'_______' ## Complete the code to fit the model on over sampled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDzHR8Pwbp1D"
      },
      "outputs": [],
      "source": [
        "xgb2_train_perf =  '_______' ## Complete the code to check the performance on oversampled train set\n",
        "xgb2_train_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiDRsOrMbp1D",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "xgb2_val_perf =  '_______' ## Complete the code to check the performance on validation set\n",
        "xgb2_val_perf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJsiURQL4eBn"
      },
      "source": [
        "**We have now tuned all the models, let's compare the performance of all tuned models and see which one is the best.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9JNnpxa4jau"
      },
      "source": [
        "## Model performance comparison and choosing the final model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TF1WfZZHbp1E"
      },
      "outputs": [],
      "source": [
        "# training performance comparison\n",
        "\n",
        "models_train_comp_df = pd.concat(\n",
        "    [\n",
        "        gbm_train_perf.T,\n",
        "        ada_train_perf.T,\n",
        "        rf2_train_perf.T,\n",
        "        xgb2_train_perf.T,\n",
        "    ],\n",
        "    axis=1,\n",
        ")\n",
        "models_train_comp_df.columns = [\n",
        "    \"Gradient Boosting tuned with oversampled data\",\n",
        "    \"AdaBoost classifier tuned with oversampled data\",\n",
        "    \"Random forest tuned with undersampled data\",\n",
        "    \"XGBoost tuned with oversampled data\",\n",
        "]\n",
        "print(\"Training performance comparison:\")\n",
        "models_train_comp_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FgTXpsXbp1F"
      },
      "outputs": [],
      "source": [
        "# validation performance comparison\n",
        "\n",
        "'_______' ## Write the code to compare the performance on validation set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYS5m_mcbp1F"
      },
      "source": [
        "**Now we have our final model, so let's find out how our final model is performing on unseen test data.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqgvz7e7bp1F"
      },
      "outputs": [],
      "source": [
        "# Let's check the performance on test set\n",
        "'_______' ## Write the code to check the performance of best model on test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J49s-TEB41JQ"
      },
      "source": [
        "### Feature Importances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Zk2dFRzbp1F"
      },
      "outputs": [],
      "source": [
        "feature_names = X_train.columns\n",
        "importances =  '_______' ## Complete the code to check the feature importance of the best model\n",
        "indices = np.argsort(importances)\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.title(\"Feature Importances\")\n",
        "plt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\n",
        "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
        "plt.xlabel(\"Relative Importance\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gohwGn0tbp1L"
      },
      "source": [
        "## Let's use Pipelines to build the final model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euQoHofA5JBx"
      },
      "source": [
        "- Since we have only one datatype in the data, we don't need to use column transformer here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTB5oErrbp1L"
      },
      "outputs": [],
      "source": [
        "Pipeline_model = Pipeline('_______' ) ## Complete the code to create pipeline for the best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVhk_Ga4bp1L"
      },
      "outputs": [],
      "source": [
        "# Separating target variable and other variables\n",
        "X1 = data.drop(columns=\"Target\")\n",
        "Y1 = data[\"Target\"]\n",
        "\n",
        "# Since we already have a separate test set, we don't need to divide data into train and test\n",
        "\n",
        "X_test1 = df_test.('_______') ##  Complete the code to drop target variable from test data\n",
        "y_test1 = df_test['_______'] ##  Complete the code to store target variable in y_test1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esCUwy3ibp1L"
      },
      "outputs": [],
      "source": [
        "# We can't oversample/undersample data without doing missing value treatment, so let's first treat the missing values in the train set\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "X1 = imputer.fit_transform(X1)\n",
        "\n",
        "# We don't need to impute missing values in test set as it will be done inside pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKYy2r85zBB7"
      },
      "source": [
        "**Note:** Please perform either oversampling or undersampling based on the final model chosen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILVQdGr2KNO0"
      },
      "source": [
        "If the best model is built on the oversampled data, uncomment and run the below code to perform oversampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRhGqGEqbp1M"
      },
      "outputs": [],
      "source": [
        "# #code for oversampling on the data\n",
        "# # Synthetic Minority Over Sampling Technique\n",
        "# sm = SMOTE(sampling_strategy=1, k_neighbors=5, random_state=1)\n",
        "# X_over1, y_over1 = sm.fit_resample(X1, Y1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFESt-LQKo_5"
      },
      "source": [
        "If the best model is built on the undersampled data, uncomment and run the below code to perform undersampling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1S2b6yUKmAz"
      },
      "outputs": [],
      "source": [
        "# # code for undersampling on the data\n",
        "# # Under Sampling Technique\n",
        "# rus = RandomUnderSampler(random_state=1, sampling_strategy=1)\n",
        "# X_train_un, y_train_un = rus.fit_resample(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWI12Lh5bp1M"
      },
      "outputs": [],
      "source": [
        "Pipeline_model.'_______' ##  Complete the code to fit the Model obtained from above step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlKakNj7592p"
      },
      "outputs": [],
      "source": [
        "Pipeline_model_test = '_______'  ## Complete the code to check the performance on test set\n",
        "Pipeline_model_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxFmwam_bp1M"
      },
      "source": [
        "# Business Insights and Conclusions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc5SEIuJnyQm"
      },
      "source": [
        "- Best model and its performance\n",
        "- Important features\n",
        "- Additional points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyLBn7uE6LhP"
      },
      "source": [
        "***"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "v_-uuGqH-qTt",
        "xxhpZv9y-qTw",
        "KQi5ygTC-qT1",
        "961px703qhLV",
        "5TcqcxbK-qT3",
        "xNr4bWoM-qT5",
        "Ch_TjRfF-qT5",
        "nUCorhch-qT4",
        "DhPuzWO7hmV8",
        "Lv7Bs8aUbp07",
        "sfvnghrACelD",
        "Bp8vC9MZbp09",
        "0J99-7Kubp09",
        "8Je_G2zN3VCF",
        "C91_6Swtbp1A",
        "fYLfDmHvbp1B",
        "2FtmPS7Ubp1D",
        "Dqj6dc38bp1E",
        "XoLT8ewJ5V2d",
        "D9JNnpxa4jau",
        "gohwGn0tbp1L",
        "gxFmwam_bp1M"
      ],
      "name": "MT_Project_LearnerNotebook_LowCode_MT_002.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
